{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12c624c",
   "metadata": {},
   "source": [
    "# 生成long-format格式文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f65588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TNB_CSV = \"/work/longyh/BY/processed/TNB/TNB_summary_by_unit.csv\"\n",
    "CLINICAL_XLSX = \"/work/longyh/BY/raw/1-s2.0-S0092867417311224-mmc2.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ce8c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged n = 22 | responders = 6 / 22\n",
      "Long-format rows = 528\n",
      "  sample  tnb_value      unit strategy which            strategy_name Patient  \\\n",
      "0   Pt10        124  mutation       s1   raw  Binding-only (IC50<500)    Pt10   \n",
      "1   Pt11         82  mutation       s1   raw  Binding-only (IC50<500)    Pt11   \n",
      "2   Pt18        241  mutation       s1   raw  Binding-only (IC50<500)    Pt18   \n",
      "3   Pt24          1  mutation       s1   raw  Binding-only (IC50<500)    Pt24   \n",
      "4   Pt27        190  mutation       s1   raw  Binding-only (IC50<500)    Pt27   \n",
      "5   Pt28         17  mutation       s1   raw  Binding-only (IC50<500)    Pt28   \n",
      "6   Pt29        521  mutation       s1   raw  Binding-only (IC50<500)    Pt29   \n",
      "7   Pt30         36  mutation       s1   raw  Binding-only (IC50<500)    Pt30   \n",
      "8   Pt31        343  mutation       s1   raw  Binding-only (IC50<500)    Pt31   \n",
      "9    Pt3        128  mutation       s1   raw  Binding-only (IC50<500)     Pt3   \n",
      "\n",
      "  Response  response_label  Mutation Load  Neo-antigen Load  Neo-peptide Load  \\\n",
      "0       SD               0           75.0              33.0              56.0   \n",
      "1       PD               0          106.0              67.0             187.0   \n",
      "2       PR               1          217.0             137.0             311.0   \n",
      "3       PD               0            1.0               1.0               1.0   \n",
      "4       PD               0          183.0             119.0             321.0   \n",
      "5       PD               0           42.0              16.0              26.0   \n",
      "6       PD               0          412.0             238.0             581.0   \n",
      "7       CR               1           66.0              35.0              86.0   \n",
      "8       PD               0          450.0             294.0             870.0   \n",
      "9       PR               1          182.0             119.0             346.0   \n",
      "\n",
      "   Cytolytic Score  Dead/Alive\\n(Dead = True)  Time to Death\\n(weeks)  \\\n",
      "0        65.840717                       True               36.571429   \n",
      "1       602.674041                       True              119.571429   \n",
      "2       907.568179                      False              153.285714   \n",
      "3       423.336745                      False               21.285714   \n",
      "4       233.666429                       True               67.857143   \n",
      "5       277.479729                       True              105.714286   \n",
      "6       207.889394                       True               39.000000   \n",
      "7        64.969223                      False              150.428571   \n",
      "8       368.903781                      False              137.285714   \n",
      "9        36.331804                      False              163.428571   \n",
      "\n",
      "       Cohort  \n",
      "0  NIV3-NAIVE  \n",
      "1  NIV3-NAIVE  \n",
      "2  NIV3-NAIVE  \n",
      "3  NIV3-NAIVE  \n",
      "4  NIV3-NAIVE  \n",
      "5  NIV3-NAIVE  \n",
      "6  NIV3-NAIVE  \n",
      "7  NIV3-NAIVE  \n",
      "8  NIV3-NAIVE  \n",
      "9  NIV3-NAIVE  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Helpers (same as before)\n",
    "# -------------------------\n",
    "def make_response_label(df, response_col=\"Response\"):\n",
    "    df = df.copy()\n",
    "    df = df[df[response_col].notna()]\n",
    "    df[response_col] = df[response_col].astype(str).str.strip().str.upper()\n",
    "    df = df[df[response_col] != \"NE\"].copy()\n",
    "    response_map = {\"CR\": 1, \"PR\": 1, \"SD\": 0, \"PD\": 0}\n",
    "    df[\"response_label\"] = df[response_col].map(response_map)\n",
    "\n",
    "    unknown = df.loc[df[\"response_label\"].isna(), response_col].value_counts()\n",
    "    if len(unknown) > 0:\n",
    "        print(\"[WARN] Unmapped Response categories:\\n\", unknown)\n",
    "        df = df.dropna(subset=[\"response_label\"]).copy()\n",
    "\n",
    "    df[\"response_label\"] = df[\"response_label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def wide_to_long_tnb(df, id_cols=(\"sample\",), pattern=r\"^(mutation|peptide|hla_peptide)_(s[1-4])_(raw|unique)$\"):\n",
    "    \"\"\"\n",
    "    Convert wide TNB columns (e.g., mutation_s1_unique) to long format:\n",
    "      columns: sample, unit, strategy, which, tnb_value\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    rx = re.compile(pattern)\n",
    "\n",
    "    tnb_cols = [c for c in df.columns if rx.match(c)]\n",
    "    if not tnb_cols:\n",
    "        raise ValueError(\"No TNB columns matched the expected pattern. Check your column names.\")\n",
    "\n",
    "    # melt\n",
    "    long = df.melt(\n",
    "        id_vars=list(id_cols),\n",
    "        value_vars=tnb_cols,\n",
    "        var_name=\"tnb_key\",\n",
    "        value_name=\"tnb_value\",\n",
    "    )\n",
    "\n",
    "    # parse key\n",
    "    m = long[\"tnb_key\"].str.extract(pattern)\n",
    "    m.columns = [\"unit\", \"strategy\", \"which\"]\n",
    "    long = pd.concat([long.drop(columns=[\"tnb_key\"]), m], axis=1)\n",
    "\n",
    "    # numeric\n",
    "    long[\"tnb_value\"] = pd.to_numeric(long[\"tnb_value\"], errors=\"coerce\")\n",
    "\n",
    "    # drop missing\n",
    "    long = long.dropna(subset=[\"tnb_value\"]).copy()\n",
    "\n",
    "    # optional: add human-readable strategy name\n",
    "    strategy_map = {\n",
    "        \"s1\": \"Binding-only (IC50<500)\",\n",
    "        \"s2\": \"IC50<500 & TPM>1\",\n",
    "        \"s3\": \"IC50<50 & TPM>5\",\n",
    "        \"s4\": \"High-quality (IC50<50,TPM>5,VAF>0.1,WT>1000)\",\n",
    "    }\n",
    "    long[\"strategy_name\"] = long[\"strategy\"].map(strategy_map).fillna(long[\"strategy\"])\n",
    "\n",
    "    return long\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1) Load data\n",
    "# -------------------------\n",
    "summary = pd.read_csv(TNB_CSV, dtype=str)\n",
    "clinical = pd.read_excel(CLINICAL_XLSX,skiprows=2)\n",
    "\n",
    "# ensure join key\n",
    "summary[\"sample\"] = summary[\"sample\"].astype(str).str.strip()\n",
    "clinical[\"Patient\"] = clinical[\"Patient\"].astype(str).str.strip()\n",
    "\n",
    "# numeric conversion in summary (except sample/file)\n",
    "for c in summary.columns:\n",
    "    if c in [\"sample\", \"file\"]:\n",
    "        continue\n",
    "    summary[c] = pd.to_numeric(summary[c], errors=\"coerce\")\n",
    "\n",
    "# clinical: response label + drop NE\n",
    "clinical2 = make_response_label(clinical, response_col=\"Response\")\n",
    "\n",
    "# merge\n",
    "merged = pd.merge(summary, clinical2, left_on=\"sample\", right_on=\"Patient\", how=\"inner\")\n",
    "print(\"Merged n =\", len(merged), \"| responders =\", merged[\"response_label\"].sum(), \"/\", len(merged))\n",
    "\n",
    "# -------------------------\n",
    "# 2) Wide -> Long (TNB)\n",
    "# -------------------------\n",
    "# keep whichever clinical cols you care about\n",
    "keep_cols = [\n",
    "    \"sample\", \"Patient\", \"Response\", \"response_label\",\n",
    "    \"Mutation Load\", \"Neo-antigen Load\", \"Neo-peptide Load\", \"Cytolytic Score\",\n",
    "    \"Dead/Alive\\n(Dead = True)\", \"Time to Death\\n(weeks)\",\n",
    "    \"Cohort\"\n",
    "]\n",
    "keep_cols = [c for c in keep_cols if c in merged.columns]\n",
    "\n",
    "merged_small = merged[keep_cols + [c for c in merged.columns if re.match(r\"^(mutation|peptide|hla_peptide)_s[1-4]_(raw|unique)$\", c)]].copy()\n",
    "\n",
    "tnb_long = wide_to_long_tnb(merged_small, id_cols=(\"sample\",))\n",
    "# attach clinical columns (wide_to_long_tnb keeps only id_cols by default)\n",
    "tnb_long = tnb_long.merge(merged_small[keep_cols].drop_duplicates(\"sample\"), on=\"sample\", how=\"left\")\n",
    "\n",
    "print(\"Long-format rows =\", len(tnb_long))\n",
    "print(tnb_long.head(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df3b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3) Save\n",
    "# -------------------------\n",
    "# tnb_long.to_csv(\"/work/longyh/BY/processed/TNB/tnb_long_format.csv\", index=False)\n",
    "# print(\"Saved: tnb_long_format.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d4f29",
   "metadata": {},
   "source": [
    "# 使用long-format 文件生成统计结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e88a86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# lifelines for KM/Cox\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "from lifelines.statistics import logrank_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c610abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 0) EDIT HERE\n",
    "# =========================\n",
    "LONG_CSV = \"/work/longyh/BY/processed/TNB/tnb_long_format.csv\"  # 你的long-format文件\n",
    "OUTDIR = \"/work/longyh/BY/plots\"\n",
    "COHORT_LABEL = \"Ipi-N\"  # 纯标签：不依赖临床Cohort列\n",
    "FILTER_WHICH = [\"unique\"]  # 可改成 [\"unique\"] 或 [\"raw\",\"unique\"]\n",
    "FILTER_STRATEGY = [\"s1\", \"s2\", \"s3\", \"s4\"]  # 可选：只跑 [\"s1\"]\n",
    "FILTER_UNIT = [\"mutation\", \"peptide\", \"hla_peptide\"]\n",
    "N_BOOT = 2000\n",
    "SEED = 1\n",
    "\n",
    "# 是否保存图片\n",
    "SAVE_PLOTS = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e18b3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 1) Helpers\n",
    "# =========================\n",
    "def ensure_response_label(df):\n",
    "    \"\"\"\n",
    "    Ensure response_label exists: CR/PR=1, SD/PD=0, NE removed.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if \"response_label\" in df.columns and df[\"response_label\"].notna().any():\n",
    "        # make sure int\n",
    "        df = df.dropna(subset=[\"response_label\"])\n",
    "        df[\"response_label\"] = df[\"response_label\"].astype(int)\n",
    "        return df\n",
    "\n",
    "    if \"Response\" not in df.columns:\n",
    "        raise ValueError(\"Neither response_label nor Response column found.\")\n",
    "\n",
    "    d = df.copy()\n",
    "    d[\"Response\"] = d[\"Response\"].astype(str).str.strip().str.upper()\n",
    "    d = d[d[\"Response\"] != \"NE\"].copy()\n",
    "    mp = {\"CR\": 1, \"PR\": 1, \"SD\": 0, \"PD\": 0}\n",
    "    d[\"response_label\"] = d[\"Response\"].map(mp)\n",
    "    unknown = d.loc[d[\"response_label\"].isna(), \"Response\"].value_counts()\n",
    "    if len(unknown) > 0:\n",
    "        print(\"[WARN] Unmapped Response categories:\\n\", unknown)\n",
    "        d = d.dropna(subset=[\"response_label\"]).copy()\n",
    "    d[\"response_label\"] = d[\"response_label\"].astype(int)\n",
    "    return d\n",
    "\n",
    "\n",
    "def bootstrap_auc_ci(y_true, y_score, n_boot=2000, seed=1):\n",
    "    \"\"\"\n",
    "    Bootstrap percentile CI for AUC.\n",
    "    Skips resamples with single class.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_score = np.asarray(y_score, dtype=float)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc_point = auc(fpr, tpr)\n",
    "\n",
    "    boot = []\n",
    "    n = len(y_true)\n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        yt = y_true[idx]\n",
    "        ys = y_score[idx]\n",
    "        if len(np.unique(yt)) < 2:\n",
    "            continue\n",
    "        fpr_b, tpr_b, _ = roc_curve(yt, ys)\n",
    "        boot.append(auc(fpr_b, tpr_b))\n",
    "\n",
    "    if len(boot) < 50:\n",
    "        return float(auc_point), np.nan, np.nan, len(boot)\n",
    "\n",
    "    lo, hi = np.quantile(boot, [0.025, 0.975])\n",
    "    return float(auc_point), float(lo), float(hi), len(boot)\n",
    "\n",
    "\n",
    "def safe_filename(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"[^\\w\\-.]+\", \"_\", s)\n",
    "    return s[:200]\n",
    "\n",
    "\n",
    "def find_time_event_cols(df):\n",
    "    \"\"\"\n",
    "    Prefer PFS if columns exist; else OS using:\n",
    "      Time to Death (weeks) + Dead=True\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # Look for any plausible PFS\n",
    "    pfs_time_candidates = [c for c in cols if re.search(r\"\\bPFS\\b|progression\", str(c), flags=re.I)]\n",
    "    pfs_event_candidates = [c for c in cols if re.search(r\"event|status|progress\", str(c), flags=re.I)]\n",
    "\n",
    "    # heuristics: time column should contain 'week' or 'time'\n",
    "    time_col = None\n",
    "    event_col = None\n",
    "    for c in pfs_time_candidates:\n",
    "        if re.search(r\"week|time\", str(c), flags=re.I):\n",
    "            time_col = c\n",
    "            break\n",
    "    for c in pfs_event_candidates:\n",
    "        if re.search(r\"event|status\", str(c), flags=re.I):\n",
    "            event_col = c\n",
    "            break\n",
    "\n",
    "    if time_col and event_col:\n",
    "        return time_col, event_col, \"PFS\"\n",
    "\n",
    "    # fallback to your known OS columns\n",
    "    os_time = \"Time to Death\\n(weeks)\" if \"Time to Death\\n(weeks)\" in cols else None\n",
    "    os_event = \"Dead/Alive\\n(Dead = True)\" if \"Dead/Alive\\n(Dead = True)\" in cols else None\n",
    "    if os_time and os_event:\n",
    "        return os_time, os_event, \"OS\"\n",
    "\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def to_event01(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, (bool, np.bool_)):\n",
    "        return int(bool(x))\n",
    "    s = str(x).strip().lower()\n",
    "    if s in {\"true\", \"1\", \"dead\", \"yes\", \"event\"}:\n",
    "        return 1\n",
    "    if s in {\"false\", \"0\", \"alive\", \"no\", \"censored\", \"none\"}:\n",
    "        return 0\n",
    "    try:\n",
    "        v = int(float(s))\n",
    "        return 1 if v != 0 else 0\n",
    "    except Exception:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4529f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-format usable rows: 264\n",
      "Unique samples: 22\n",
      "Responders: 6 / 22\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2) Load long-format\n",
    "# =========================\n",
    "df = pd.read_csv(LONG_CSV)\n",
    "\n",
    "required = {\"sample\", \"unit\", \"strategy\", \"which\", \"tnb_value\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in long CSV: {missing}\")\n",
    "\n",
    "df[\"sample\"] = df[\"sample\"].astype(str).str.strip()\n",
    "df[\"unit\"] = df[\"unit\"].astype(str).str.strip()\n",
    "df[\"strategy\"] = df[\"strategy\"].astype(str).str.strip()\n",
    "df[\"which\"] = df[\"which\"].astype(str).str.strip()\n",
    "df[\"tnb_value\"] = pd.to_numeric(df[\"tnb_value\"], errors=\"coerce\")\n",
    "\n",
    "# add/ensure response_label and drop NE\n",
    "df = ensure_response_label(df)\n",
    "df = df.dropna(subset=[\"tnb_value\"]).copy()\n",
    "\n",
    "# optional filters\n",
    "df = df[df[\"which\"].isin(FILTER_WHICH)].copy()\n",
    "df = df[df[\"strategy\"].isin(FILTER_STRATEGY)].copy()\n",
    "df = df[df[\"unit\"].isin(FILTER_UNIT)].copy()\n",
    "\n",
    "print(\"Long-format usable rows:\", len(df))\n",
    "print(\"Unique samples:\", df[\"sample\"].nunique())\n",
    "print(\"Responders:\", int(df.drop_duplicates(\"sample\")[\"response_label\"].sum()), \"/\", df[\"sample\"].nunique())\n",
    "\n",
    "# output dir\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1618ac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: auc_summary_long.csv\n",
      "           unit strategy   which   n  responders       auc    ci_low  \\\n",
      "4      mutation       s1  unique  22           6  0.583333  0.223498   \n",
      "9       peptide       s2  unique  22           6  0.578125  0.242279   \n",
      "1   hla_peptide       s2  unique  22           6  0.567708  0.224881   \n",
      "0   hla_peptide       s1  unique  22           6  0.562500  0.227616   \n",
      "8       peptide       s1  unique  22           6  0.562500  0.223529   \n",
      "5      mutation       s2  unique  22           6  0.557292  0.221875   \n",
      "10      peptide       s3  unique  22           6  0.552083  0.213445   \n",
      "6      mutation       s3  unique  22           6  0.552083  0.200000   \n",
      "2   hla_peptide       s3  unique  22           6  0.546875  0.200850   \n",
      "3   hla_peptide       s4  unique  22           6  0.500000  0.225000   \n",
      "11      peptide       s4  unique  22           6  0.500000  0.225000   \n",
      "7      mutation       s4  unique  22           6  0.494792  0.224722   \n",
      "\n",
      "     ci_high  boot_kept  suggest_flip  \n",
      "4   0.906853       1997         False  \n",
      "9   0.894846       1997         False  \n",
      "1   0.894118       1997         False  \n",
      "0   0.877193       1997         False  \n",
      "8   0.877193       1997         False  \n",
      "5   0.886192       1997         False  \n",
      "10  0.882451       1997         False  \n",
      "6   0.880952       1997         False  \n",
      "2   0.880283       1997         False  \n",
      "3   0.780702       1997         False  \n",
      "11  0.780702       1997         False  \n",
      "7   0.771930       1997          True  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 3) AUC loop + ROC plots\n",
    "# =========================\n",
    "auc_rows = []\n",
    "\n",
    "group_cols = [\"unit\", \"strategy\", \"which\"]\n",
    "for (unit, strategy, which), g in df.groupby(group_cols):\n",
    "    # g is multiple rows across samples; each sample should appear once per (unit,strategy,which)\n",
    "    # If duplicates exist, keep first and warn\n",
    "    gg = g.sort_values(\"sample\").drop_duplicates(subset=[\"sample\"])\n",
    "    y = gg[\"response_label\"].astype(int).values\n",
    "    x = gg[\"tnb_value\"].astype(float).values\n",
    "\n",
    "    # skip invalid\n",
    "    if len(gg) < 8 or len(np.unique(y)) < 2:\n",
    "        auc_rows.append({\n",
    "            \"unit\": unit, \"strategy\": strategy, \"which\": which,\n",
    "            \"n\": int(len(gg)),\n",
    "            \"responders\": int(y.sum()),\n",
    "            \"auc\": np.nan, \"ci_low\": np.nan, \"ci_high\": np.nan,\n",
    "            \"boot_kept\": 0,\n",
    "            \"suggest_flip\": np.nan,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    auc_point, ci_low, ci_high, boot_kept = bootstrap_auc_ci(y, x, n_boot=N_BOOT, seed=SEED)\n",
    "\n",
    "    # quick direction hint (not auto flipping)\n",
    "    auc_rev, _, _, _ = bootstrap_auc_ci(y, -x, n_boot=500, seed=SEED)\n",
    "    suggest_flip = bool(auc_point < 0.5 and auc_rev > auc_point)\n",
    "\n",
    "    auc_rows.append({\n",
    "        \"unit\": unit, \"strategy\": strategy, \"which\": which,\n",
    "        \"n\": int(len(gg)),\n",
    "        \"responders\": int(y.sum()),\n",
    "        \"auc\": float(auc_point),\n",
    "        \"ci_low\": float(ci_low) if not np.isnan(ci_low) else np.nan,\n",
    "        \"ci_high\": float(ci_high) if not np.isnan(ci_high) else np.nan,\n",
    "        \"boot_kept\": int(boot_kept),\n",
    "        \"suggest_flip\": suggest_flip,\n",
    "    })\n",
    "\n",
    "    # ROC plot\n",
    "    fpr, tpr, _ = roc_curve(y, x)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    ci_text = \"CI unavailable\" if np.isnan(ci_low) else f\"95%CI [{ci_low:.3f}, {ci_high:.3f}]\"\n",
    "    flip_text = \" (maybe flip sign)\" if suggest_flip else \"\"\n",
    "    plt.title(f\"[{COHORT_LABEL}] {unit}-{strategy}-{which}\\nAUC={auc_point:.3f}, {ci_text}{flip_text}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if SAVE_PLOTS:\n",
    "        fn = safe_filename(f\"ROC_{COHORT_LABEL}_{unit}_{strategy}_{which}.png\")\n",
    "        plt.savefig(os.path.join(OUTDIR, fn), dpi=200)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "auc_summary = pd.DataFrame(auc_rows).sort_values([\"auc\"], ascending=False)\n",
    "auc_summary.to_csv(\"/work/longyh/BY/processed/TNB/auc_summary_long.csv\", index=False)\n",
    "print(\"Saved: auc_summary_long.csv\")\n",
    "print(auc_summary.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b3e148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: auc_heatmap_table_unique.csv\n",
      "Saved: auc_heatmap_table_ci_unique.csv\n",
      "Saved: auc_heatmap_table_ciwidth_unique.csv\n",
      "Saved: /work/longyh/BY/plots/AUC_heatmap_Ipi-N_unique.png\n",
      "Saved: /work/longyh/BY/plots/AUC_heatmap_CIwidth_Ipi-N_unique.png\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3.5) AUC heatmap tables + plots\n",
    "# =========================\n",
    "def plot_heatmap(matrix_df, title, xlabel, ylabel, out_png):\n",
    "    \"\"\"\n",
    "    Simple matplotlib heatmap for a pivot table (values numeric).\n",
    "    \"\"\"\n",
    "    # ensure order\n",
    "    mat = matrix_df.copy()\n",
    "\n",
    "    plt.figure(figsize=(1.2 * max(4, mat.shape[1] + 1), 1.0 * max(3, mat.shape[0] + 1)))\n",
    "    im = plt.imshow(mat.values, aspect=\"auto\")\n",
    "\n",
    "    plt.xticks(range(mat.shape[1]), mat.columns, rotation=30, ha=\"right\")\n",
    "    plt.yticks(range(mat.shape[0]), mat.index)\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.ax.set_ylabel(\"Value\", rotation=270, labelpad=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# 只用“有auc”的行\n",
    "auc_ok = auc_summary.dropna(subset=[\"auc\"]).copy()\n",
    "\n",
    "# 生成每个 which 一张热图（如果你只跑 unique，这里也只会出一张）\n",
    "for which in sorted(auc_ok[\"which\"].unique()):\n",
    "    sub = auc_ok[auc_ok[\"which\"] == which].copy()\n",
    "\n",
    "    # AUC pivot\n",
    "    auc_pivot = sub.pivot_table(\n",
    "        index=\"strategy\",\n",
    "        columns=\"unit\",\n",
    "        values=\"auc\",\n",
    "        aggfunc=\"mean\"\n",
    "    ).reindex(index=FILTER_STRATEGY, columns=FILTER_UNIT)\n",
    "\n",
    "    auc_pivot.to_csv(f\"auc_heatmap_table_{which}.csv\")\n",
    "    print(f\"Saved: auc_heatmap_table_{which}.csv\")\n",
    "\n",
    "    # AUC with CI string pivot (nice for tables)\n",
    "    sub[\"auc_ci_str\"] = sub.apply(\n",
    "        lambda r: (\n",
    "            f\"{r['auc']:.3f} [{r['ci_low']:.3f}, {r['ci_high']:.3f}]\"\n",
    "            if pd.notna(r[\"ci_low\"]) and pd.notna(r[\"ci_high\"])\n",
    "            else f\"{r['auc']:.3f} [NA]\"\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    auc_ci_pivot = sub.pivot_table(\n",
    "        index=\"strategy\",\n",
    "        columns=\"unit\",\n",
    "        values=\"auc_ci_str\",\n",
    "        aggfunc=\"first\"\n",
    "    ).reindex(index=FILTER_STRATEGY, columns=FILTER_UNIT)\n",
    "\n",
    "    auc_ci_pivot.to_csv(f\"/work/longyh/BY/processed/TNB/auc_heatmap_table_ci_{which}.csv\")\n",
    "    print(f\"Saved: auc_heatmap_table_ci_{which}.csv\")\n",
    "\n",
    "    # CI width heatmap (stability proxy)\n",
    "    sub[\"ci_width\"] = sub[\"ci_high\"] - sub[\"ci_low\"]\n",
    "    ciw_pivot = sub.pivot_table(\n",
    "        index=\"strategy\",\n",
    "        columns=\"unit\",\n",
    "        values=\"ci_width\",\n",
    "        aggfunc=\"mean\"\n",
    "    ).reindex(index=FILTER_STRATEGY, columns=FILTER_UNIT)\n",
    "\n",
    "    ciw_pivot.to_csv(f\"/work/longyh/BY/processed/TNB/auc_heatmap_table_ciwidth_{which}.csv\")\n",
    "    print(f\"Saved: auc_heatmap_table_ciwidth_{which}.csv\")\n",
    "\n",
    "    # Plot heatmaps\n",
    "    if SAVE_PLOTS:\n",
    "        out_auc = os.path.join(\n",
    "            OUTDIR,\n",
    "            safe_filename(f\"AUC_heatmap_{COHORT_LABEL}_{which}.png\")\n",
    "        )\n",
    "        plot_heatmap(\n",
    "            auc_pivot,\n",
    "            title=f\"[{COHORT_LABEL}] AUC heatmap ({which})\",\n",
    "            xlabel=\"Counting unit\",\n",
    "            ylabel=\"Strategy\",\n",
    "            out_png=out_auc\n",
    "        )\n",
    "        print(\"Saved:\", out_auc)\n",
    "\n",
    "        out_ciw = os.path.join(\n",
    "            OUTDIR,\n",
    "            safe_filename(f\"AUC_heatmap_CIwidth_{COHORT_LABEL}_{which}.png\")\n",
    "        )\n",
    "        plot_heatmap(\n",
    "            ciw_pivot,\n",
    "            title=f\"[{COHORT_LABEL}] AUC CI width heatmap ({which})\",\n",
    "            xlabel=\"Counting unit\",\n",
    "            ylabel=\"Strategy\",\n",
    "            out_png=out_ciw\n",
    "        )\n",
    "        print(\"Saved:\", out_ciw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0704768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using endpoint=OS | time_col=Time to Death\n",
      "(weeks) | event_col=Dead/Alive\n",
      "(Dead = True)\n",
      "KM usable samples: 22\n",
      "Saved: km_cox_summary_long.csv\n",
      "           unit strategy   which endpoint   n  events  median_cut  logrank_p  \\\n",
      "4      mutation       s1  unique       OS  22      15        30.5   0.541620   \n",
      "3   hla_peptide       s4  unique       OS  22      15         0.0   0.724310   \n",
      "11      peptide       s4  unique       OS  22      15         0.0   0.724310   \n",
      "5      mutation       s2  unique       OS  22      15        23.5   0.613692   \n",
      "1   hla_peptide       s2  unique       OS  22      15        85.0   0.613692   \n",
      "0   hla_peptide       s1  unique       OS  22      15       127.5   0.461506   \n",
      "6      mutation       s3  unique       OS  22      15         8.0   0.851040   \n",
      "8       peptide       s1  unique       OS  22      15       118.0   0.293848   \n",
      "9       peptide       s2  unique       OS  22      15        74.5   0.757195   \n",
      "7      mutation       s4  unique       OS  22      15         0.0   0.724310   \n",
      "2   hla_peptide       s3  unique       OS  22      15        13.0   0.896587   \n",
      "10      peptide       s3  unique       OS  22      15        13.0   0.896587   \n",
      "\n",
      "    cox_hr_log1p  cox_ci_low  cox_ci_high     cox_p  \n",
      "4       1.207477    0.680887     2.141328  0.518924  \n",
      "3       0.823509    0.434576     1.560528  0.551572  \n",
      "11      0.823509    0.434576     1.560528  0.551572  \n",
      "5       1.175914    0.673775     2.052277  0.568473  \n",
      "1       1.117608    0.762613     1.637853  0.568537  \n",
      "0       1.109858    0.770743     1.598180  0.575297  \n",
      "6       1.153326    0.670115     1.984973  0.606596  \n",
      "8       1.093979    0.763970     1.566542  0.623911  \n",
      "9       1.098179    0.753489     1.600550  0.626058  \n",
      "7       0.863702    0.385182     1.936697  0.722105  \n",
      "2       1.075164    0.702295     1.646000  0.738728  \n",
      "10      1.068804    0.698555     1.635293  0.759102  \n",
      "Done. Plots saved to: /work/longyh/BY/plots\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =========================\n",
    "# 4) KM + Cox loop (optional)\n",
    "# =========================\n",
    "time_col, event_col, endpoint = find_time_event_cols(df)\n",
    "km_rows = []\n",
    "\n",
    "if time_col is None:\n",
    "    print(\"[INFO] No time-to-event columns detected; skip KM/Cox.\")\n",
    "else:\n",
    "    km_df = df.copy()\n",
    "    km_df[event_col] = km_df[event_col].apply(to_event01)\n",
    "    km_df[time_col] = pd.to_numeric(km_df[time_col], errors=\"coerce\")\n",
    "    km_df = km_df.dropna(subset=[time_col, event_col]).copy()\n",
    "    km_df[event_col] = km_df[event_col].astype(int)\n",
    "\n",
    "    print(f\"[INFO] Using endpoint={endpoint} | time_col={time_col} | event_col={event_col}\")\n",
    "    print(\"KM usable samples:\", km_df.drop_duplicates([\"sample\"]).shape[0])\n",
    "\n",
    "    for (unit, strategy, which), g in km_df.groupby(group_cols):\n",
    "        gg = g.sort_values(\"sample\").drop_duplicates(subset=[\"sample\"])\n",
    "        if len(gg) < 10:\n",
    "            km_rows.append({\n",
    "                \"unit\": unit, \"strategy\": strategy, \"which\": which,\n",
    "                \"endpoint\": endpoint, \"n\": int(len(gg)),\n",
    "                \"events\": int(gg[event_col].sum()),\n",
    "                \"median_cut\": np.nan,\n",
    "                \"logrank_p\": np.nan,\n",
    "                \"cox_hr_log1p\": np.nan, \"cox_ci_low\": np.nan, \"cox_ci_high\": np.nan, \"cox_p\": np.nan,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # median split\n",
    "        median_val = gg[\"tnb_value\"].median()\n",
    "        gg = gg.copy()\n",
    "        gg[\"group\"] = np.where(gg[\"tnb_value\"] > median_val, \"High\", \"Low\")\n",
    "\n",
    "        gH = gg[gg[\"group\"] == \"High\"]\n",
    "        gL = gg[gg[\"group\"] == \"Low\"]\n",
    "        if len(gH) < 3 or len(gL) < 3:\n",
    "            km_rows.append({\n",
    "                \"unit\": unit, \"strategy\": strategy, \"which\": which,\n",
    "                \"endpoint\": endpoint, \"n\": int(len(gg)),\n",
    "                \"events\": int(gg[event_col].sum()),\n",
    "                \"median_cut\": float(median_val),\n",
    "                \"logrank_p\": np.nan,\n",
    "                \"cox_hr_log1p\": np.nan, \"cox_ci_low\": np.nan, \"cox_ci_high\": np.nan, \"cox_p\": np.nan,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # log-rank\n",
    "        lr = logrank_test(\n",
    "            gH[time_col], gL[time_col],\n",
    "            event_observed_A=gH[event_col],\n",
    "            event_observed_B=gL[event_col],\n",
    "        )\n",
    "\n",
    "        # KM plot\n",
    "        kmf = KaplanMeierFitter()\n",
    "        plt.figure(figsize=(6.8, 5.6))\n",
    "\n",
    "        kmf.fit(gL[time_col], event_observed=gL[event_col], label=f\"Low (n={len(gL)})\")\n",
    "        ax = kmf.plot(ci_show=True)\n",
    "        kmf.fit(gH[time_col], event_observed=gH[event_col], label=f\"High (n={len(gH)})\")\n",
    "        kmf.plot(ax=ax, ci_show=True)\n",
    "\n",
    "        plt.xlabel(f\"Time (weeks) [{endpoint}]\")\n",
    "        plt.ylabel(\"Survival probability\")\n",
    "        plt.title(f\"[{COHORT_LABEL}] {unit}-{strategy}-{which}\\nlog-rank p={lr.p_value:.3g} (median={median_val:.3g})\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if SAVE_PLOTS:\n",
    "            fn = safe_filename(f\"KM_{COHORT_LABEL}_{endpoint}_{unit}_{strategy}_{which}.png\")\n",
    "            plt.savefig(os.path.join(OUTDIR, fn), dpi=200)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        # Cox continuous: log1p(tnb)\n",
    "        cox_df = gg[[time_col, event_col, \"tnb_value\"]].copy()\n",
    "        cox_df[\"x\"] = np.log1p(cox_df[\"tnb_value\"].astype(float))\n",
    "\n",
    "        cph = CoxPHFitter()\n",
    "        cph.fit(cox_df[[time_col, event_col, \"x\"]], duration_col=time_col, event_col=event_col)\n",
    "        summ = cph.summary.loc[\"x\", [\"exp(coef)\", \"exp(coef) lower 95%\", \"exp(coef) upper 95%\", \"p\"]]\n",
    "\n",
    "        hr = float(summ[\"exp(coef)\"])\n",
    "        lo = float(summ[\"exp(coef) lower 95%\"])\n",
    "        hi = float(summ[\"exp(coef) upper 95%\"])\n",
    "        p = float(summ[\"p\"])\n",
    "\n",
    "        km_rows.append({\n",
    "            \"unit\": unit, \"strategy\": strategy, \"which\": which,\n",
    "            \"endpoint\": endpoint, \"n\": int(len(gg)),\n",
    "            \"events\": int(gg[event_col].sum()),\n",
    "            \"median_cut\": float(median_val),\n",
    "            \"logrank_p\": float(lr.p_value),\n",
    "            \"cox_hr_log1p\": hr, \"cox_ci_low\": lo, \"cox_ci_high\": hi, \"cox_p\": p,\n",
    "        })\n",
    "\n",
    "    km_summary = pd.DataFrame(km_rows).sort_values([\"cox_p\"])\n",
    "    km_summary.to_csv(\"/work/longyh/BY/processed/TNB/km_cox_summary_long.csv\", index=False)\n",
    "    print(\"Saved: km_cox_summary_long.csv\")\n",
    "    print(km_summary.head(20))\n",
    "\n",
    "\n",
    "print(\"Done. Plots saved to:\", OUTDIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc64719f",
   "metadata": {},
   "source": [
    "# 一致性对照：3种计数层级的相关性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a1f3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c73daaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency check rows: 264\n",
      "Samples: 22\n"
     ]
    }
   ],
   "source": [
    "# df_long: 你的 long-format DataFrame（比如读 tnb_long_format.csv）\n",
    "# 必须包含列：sample, unit, strategy, which, tnb_value\n",
    "df=pd.read_csv(\"/work/longyh/BY/processed/TNB/tnb_long_format.csv\")\n",
    "# =========================\n",
    "# 1) 选择做一致性对照的层级\n",
    "# =========================\n",
    "WHICH = \"unique\"      # 强烈建议用 unique\n",
    "COHORT_LABEL = \"Ipi-N \"  # 纯标签：不依赖临床Cohort列\n",
    "STRATEGIES = [\"s1\", \"s2\", \"s3\", \"s4\"]  # 你也可以只做 [\"s1\"]\n",
    "UNITS = [\"mutation\", \"peptide\", \"hla_peptide\"]\n",
    "PLOT_DIR = \"/work/longyh/BY/plots\"\n",
    "\n",
    "df_check = df.copy()\n",
    "df_check = df_check[df_check[\"which\"] == WHICH].copy()\n",
    "df_check = df_check[df_check[\"unit\"].isin(UNITS)].copy()\n",
    "df_check = df_check[df_check[\"strategy\"].isin(STRATEGIES)].copy()\n",
    "\n",
    "print(\"Consistency check rows:\", len(df_check))\n",
    "print(\"Samples:\", df_check[\"sample\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b71c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) 对每个 strategy：做 wide pivot + Spearman 相关\n",
    "# =========================\n",
    "\n",
    "def spearman_corr_matrix(wide_df):\n",
    "    \"\"\"\n",
    "    wide_df: columns are units, rows are samples\n",
    "    returns Spearman correlation matrix (DataFrame)\n",
    "    \"\"\"\n",
    "    return wide_df.corr(method=\"spearman\")\n",
    "\n",
    "def plot_corr_heatmap(corr_df, title):\n",
    "    plt.figure(figsize=(5.5, 4.8))\n",
    "    im = plt.imshow(corr_df.values, aspect=\"auto\")\n",
    "    plt.xticks(range(corr_df.shape[1]), corr_df.columns, rotation=30, ha=\"right\")\n",
    "    plt.yticks(range(corr_df.shape[0]), corr_df.index)\n",
    "    plt.title(title)\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.ax.set_ylabel(\"Spearman ρ\", rotation=270, labelpad=12)\n",
    "    plt.tight_layout()\n",
    "    out = f\"{PLOT_DIR}/spearman_heatmap_{COHORT_LABEL}_{WHICH}_{title.replace(' ', '_')}.png\"\n",
    "    plt.savefig(out, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_scatter_pairs(wide_df, strategy, title_prefix=\"\"):\n",
    "\n",
    "    \"\"\"\n",
    "    画三种 unit 两两散点（建议 log1p 让尺度更稳）\n",
    "    \"\"\"\n",
    "    wide2 = wide_df.copy()\n",
    "    for c in wide2.columns:\n",
    "        wide2[c] = np.log1p(wide2[c].astype(float))\n",
    "\n",
    "    pairs = [(\"mutation\", \"peptide\"), (\"mutation\", \"hla_peptide\"), (\"peptide\", \"hla_peptide\")]\n",
    "    for a, b in pairs:\n",
    "        if a not in wide2.columns or b not in wide2.columns:\n",
    "            continue\n",
    "        x = wide2[a].values\n",
    "        y = wide2[b].values\n",
    "        rho = pd.Series(x).corr(pd.Series(y), method=\"spearman\")\n",
    "\n",
    "        plt.figure(figsize=(5.5, 5.0))\n",
    "        plt.scatter(x, y, alpha=0.75)\n",
    "        plt.xlabel(f\"log1p({a})\")\n",
    "        plt.ylabel(f\"log1p({b})\")\n",
    "        plt.title(f\"{title_prefix}{a} vs {b}\\nSpearman ρ={rho:.3f}\")\n",
    "        plt.tight_layout()\n",
    "        out = f\"{PLOT_DIR}/spearman_scatter_{COHORT_LABEL}_{WHICH}_{strategy}_{a}_vs_{b}.png\"\n",
    "        plt.savefig(out, dpi=200)\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "faad7192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Strategy: s1\n",
      "Samples with all 3 units: 22\n",
      "unit         mutation   peptide  hla_peptide\n",
      "unit                                        \n",
      "mutation     1.000000  0.931299     0.939215\n",
      "peptide      0.931299  1.000000     0.980802\n",
      "hla_peptide  0.939215  0.980802     1.000000\n",
      "\n",
      "==============================\n",
      "Strategy: s2\n",
      "Samples with all 3 units: 22\n",
      "unit         mutation   peptide  hla_peptide\n",
      "unit                                        \n",
      "mutation     1.000000  0.933824     0.944869\n",
      "peptide      0.933824  1.000000     0.975982\n",
      "hla_peptide  0.944869  0.975982     1.000000\n",
      "\n",
      "==============================\n",
      "Strategy: s3\n",
      "Samples with all 3 units: 22\n",
      "unit         mutation   peptide  hla_peptide\n",
      "unit                                        \n",
      "mutation     1.000000  0.937646     0.945049\n",
      "peptide      0.937646  1.000000     0.998296\n",
      "hla_peptide  0.945049  0.998296     1.000000\n",
      "\n",
      "==============================\n",
      "Strategy: s4\n",
      "Samples with all 3 units: 22\n",
      "unit         mutation   peptide  hla_peptide\n",
      "unit                                        \n",
      "mutation     1.000000  0.994284     0.994284\n",
      "peptide      0.994284  1.000000     1.000000\n",
      "hla_peptide  0.994284  1.000000     1.000000\n",
      "\n",
      "Saved: tnb_unit_spearman_summary.csv\n",
      "  strategy   which  n_complete  rho_mut_pep  rho_mut_hla  rho_pep_hla\n",
      "0       s1  unique          22     0.931299     0.939215     0.980802\n",
      "1       s2  unique          22     0.933824     0.944869     0.975982\n",
      "2       s3  unique          22     0.937646     0.945049     0.998296\n",
      "3       s4  unique          22     0.994284     0.994284     1.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 保存结果表（论文很有用）\n",
    "corr_rows = []\n",
    "\n",
    "for s in STRATEGIES:\n",
    "    g = df_check[df_check[\"strategy\"] == s].copy()\n",
    "\n",
    "    # pivot to samples x unit\n",
    "    wide = g.pivot_table(index=\"sample\", columns=\"unit\", values=\"tnb_value\", aggfunc=\"first\")\n",
    "    # 有些样本可能缺失某个 unit（比如h1a_peptide缺失），这里只用共同样本做相关\n",
    "    wide_common = wide.dropna(subset=UNITS, how=\"any\").copy()\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Strategy:\", s)\n",
    "    print(\"Samples with all 3 units:\", len(wide_common))\n",
    "\n",
    "    if len(wide_common) < 8:\n",
    "        print(\"[SKIP] too few complete samples for correlation.\")\n",
    "        continue\n",
    "\n",
    "    corr = spearman_corr_matrix(wide_common[UNITS])\n",
    "    print(corr)\n",
    "\n",
    "    # 记录到表\n",
    "    corr_rows.append({\n",
    "        \"strategy\": s,\n",
    "        \"which\": WHICH,\n",
    "        \"n_complete\": int(len(wide_common)),\n",
    "        \"rho_mut_pep\": float(corr.loc[\"mutation\", \"peptide\"]),\n",
    "        \"rho_mut_hla\": float(corr.loc[\"mutation\", \"hla_peptide\"]),\n",
    "        \"rho_pep_hla\": float(corr.loc[\"peptide\", \"hla_peptide\"]),\n",
    "    })\n",
    "\n",
    "    # 画相关热图 + 两两散点（都很直观）\n",
    "    plot_corr_heatmap(corr, title=f\"[{COHORT_LABEL}] Spearman correlation (which={WHICH}, {s})\")\n",
    "    \n",
    "    plot_scatter_pairs(\n",
    "    wide_common[UNITS],\n",
    "    strategy=s,\n",
    "    title_prefix=f\"[{COHORT_LABEL}] which={WHICH}, {s}: \"\n",
    ")\n",
    "\n",
    "# 汇总表输出\n",
    "corr_summary = pd.DataFrame(corr_rows)\n",
    "corr_summary.to_csv(\"/work/longyh/BY/processed/TNB/tnb_unit_spearman_summary.csv\", index=False)\n",
    "print(\"\\nSaved: tnb_unit_spearman_summary.csv\")\n",
    "print(corr_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
