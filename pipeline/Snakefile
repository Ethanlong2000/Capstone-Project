configfile: "config.yaml"

NEO_ENV = "/home/longyh/miniforge3/envs/neo"  # 替换为你的neo环境路径
PVACTOOLS_ENV = "/home/longyh/miniforge3/envs/pvactools"  # 替换为你的pvactools路径
OPTITYPE_ENV = "/home/longyh/miniforge3/envs/optitype"  # OptiType 环境路径

# 全局参数校验
import os, glob, re
assert os.path.exists(config["ref_genome"]), f"Ref genome {config['ref_genome']} not found!"
assert os.path.exists(config["germline_resource"]), "Germline resource not found!"
assert os.path.exists(config["panel_of_normals"]), "PON not found!"
assert "epitope_lengths" in config["pvacseq"], "请在config.yaml的pvacseq下配置epitope_lengths（如8,9,10,11）"
assert "netmhc_dir" in config, "请在config.yaml中配置netmhc_dir路径"
assert "genome_intervals" in config, "请在config.yaml中配置genome_intervals（手动指定区间文件列表）"
assert "vep" in config, "请在config.yaml中添加vep配置块"
assert "cache_dir" in config["vep"], "请在config.yaml的vep下配置cache_dir (VEP缓存目录)"
assert "assembly" in config["vep"], "请在config.yaml的vep下配置assembly (如 GRCh38 或 GRCh37)"
assert os.path.exists(config["vep"]["cache_dir"]), f"VEP cache_dir {config['vep']['cache_dir']} 不存在！"


# 自动发现样本组（当 samples 为 'auto' 或空时）
def autodiscover_samples(data_dir="data"):
    groups = {}
    for r1 in sorted(glob.glob(os.path.join(data_dir, "*_tumor_1.fastq.gz"))):
        base = os.path.basename(r1)
        grp = re.sub(r"_tumor_1\.fastq\.gz$", "", base)
        t1 = os.path.join(data_dir, f"{grp}_tumor_1.fastq.gz")
        t2 = os.path.join(data_dir, f"{grp}_tumor_2.fastq.gz")
        n1 = os.path.join(data_dir, f"{grp}_normal_1.fastq.gz")
        n2 = os.path.join(data_dir, f"{grp}_normal_2.fastq.gz")
        if all(os.path.exists(p) for p in (t1, t2, n1, n2)):
            # 文件名中样本名就是 tumor/normal
            groups[grp] = {"tumor": "tumor", "normal": "normal"}
    return groups

if not isinstance(config.get("samples", {}), dict) or config.get("samples") in (None, "auto") or len(config.get("samples", {})) == 0:
    config["samples"] = autodiscover_samples()
    assert config["samples"], "自动发现失败：请确保 data/ 下存在 {group}_{tumor|normal}_{1,2}.fastq.gz 成对文件。"

# 提取所有样本组名（从config或自动发现）
# 自动发现/读取完成后
ALL_GROUPS = list(config["samples"].keys())

# 允许用 --config sample_groups=Pt8,Pt9 或环境变量 SAMPLE_GROUPS=Pt8,Pt9 来筛选
_selected = config.get("sample_groups") or os.environ.get("SAMPLE_GROUPS")
if _selected:
    SAMPLE_GROUPS = [g.strip() for g in str(_selected).split(",") if g.strip()]
    unknown = [g for g in SAMPLE_GROUPS if g not in ALL_GROUPS]
    assert not unknown, f"未知样本组: {unknown}；可选: {ALL_GROUPS}"
else:
    SAMPLE_GROUPS = ALL_GROUPS

rule all:
    input:
        # 每个样本组的结果文件都带样本组名
        expand("results/{sample_group}/final.neoantigens.tsv", sample_group=SAMPLE_GROUPS),
        expand("results/{sample_group}/ic50_binding_affinity.tsv", sample_group=SAMPLE_GROUPS),
        # 每个样本组的fastqc结果（tumor/normal都带样本组名）
        expand("qc/fastqc/{sample_group}_{sample}_1_fastqc.html", 
               sample_group=SAMPLE_GROUPS, sample=["tumor", "normal"])

# 1. 质控：FastQC
rule fastqc:
    input:
        r1 = "data/{sample_group}_{sample}_1.fastq.gz",
        r2 = "data/{sample_group}_{sample}_2.fastq.gz"
    output:
        html1 = "qc/fastqc/{sample_group}_{sample}_1_fastqc.html",
        html2 = "qc/fastqc/{sample_group}_{sample}_2_fastqc.html"
    threads: config["threads"]["fastqc"]
    log: "logs/{sample_group}/fastqc/{sample}.log"
    shell:
        """
        set -euo pipefail
        mkdir -p $(dirname {log}) qc/fastqc/
        fastqc -t {threads} {input.r1} {input.r2} -o qc/fastqc/ 2>&1 | tee {log}
        """

# 2. 比对 + 添加 Read Group + 标记重复（优化临时文件名、日志、内存配置）
rule align_and_dedup:
    input:
        r1 = "data/{sample_group}_{sample}_1.fastq.gz",
        r2 = "data/{sample_group}_{sample}_2.fastq.gz"
    output:
        bam = "bam/{sample_group}/{sample}.sorted.dedup.rg.bam",
        bai = "bam/{sample_group}/{sample}.sorted.dedup.rg.bam.bai"
    threads: config["threads"]["align"]
    params:
        ref = config["ref_genome"],
        tmp_dir = "tmp/{sample_group}/{sample}",
        tmp_sorted = "tmp/{sample_group}/{sample}/aligned.sorted.bam",
        tmp_rg = "tmp/{sample_group}/{sample}/aligned.sorted.rg.bam",
        rg_id = "{sample_group}_{sample}_RG001",
        rg_sm = "{sample_group}_{sample}",   # 修正：使肿瘤/正常的SM不同
        rg_lb = "{sample_group}_{sample}_Lib",
        rg_pl = "ILLUMINA",
        rg_pu = "{sample_group}_{sample}_PU001",
        gatk_mem = config.get("mem", {}).get("gatk", "64G")
    log: "logs/{sample_group}/align/{sample}.log"
    shell:
        """
        set -euo pipefail
        # 创建临时目录和输出目录（确保存在）
        mkdir -p {params.tmp_dir} $(dirname {log}) bam/{wildcards.sample_group}/

        echo "=== 步骤1：BWA-MEM2比对 + Sambamba排序 ===" >> {log} 2>&1
        # 比对→转BAM→排序：输出到独立临时文件（tmp_sorted）
        bwa-mem2 mem -t {threads} {params.ref} {input.r1} {input.r2} | \
        sambamba view -S -f bam /dev/stdin | \
        sambamba sort -t {threads} --tmpdir {params.tmp_dir} -o {params.tmp_sorted} --compression-level 6 /dev/stdin >> {log} 2>&1

        echo "=== 步骤2：GATK添加Read Group（RG） ===" >> {log} 2>&1
        # 动态内存配置：利用服务器800G内存，避免OOM
        gatk --java-options "-Xmx{params.gatk_mem} -XX:+UseParallelGC" AddOrReplaceReadGroups \
            -I {params.tmp_sorted} \
            -O {params.tmp_rg} \
            -RGID {params.rg_id} \
            -RGSM {params.rg_sm} \
            -RGLB {params.rg_lb} \
            -RGPL {params.rg_pl} \
            -RGPU {params.rg_pu} \
            >> {log} 2>&1

        echo "=== 步骤3：Sambamba标记重复 ===" >> {log} 2>&1
        # 标记重复：输入带RG的临时文件，输出到最终BAM
        sambamba markdup -t {threads} --remove-duplicates {params.tmp_rg} {output.bam} >> {log} 2>&1

        echo "=== 步骤4：建立BAM索引 ===" >> {log} 2>&1
        sambamba index -t {threads} {output.bam} >> {log} 2>&1

        echo "=== 步骤5：清理临时文件 ===" >> {log} 2>&1
        rm -f {params.tmp_sorted} {params.tmp_rg}
        rm -rf {params.tmp_dir}

        echo "=== 完成：BAM文件路径 = {output.bam} ===" >> {log} 2>&1
        # 验证RG是否添加成功（日志中记录结果）
        samtools view -H {output.bam} | grep '@RG' >> {log} 2>&1 || echo "WARNING: RG信息未找到！" >> {log} 2>&1
        """



#3 HLA分型
rule hla_typing:
    input:
        r1 = "data/{sample_group}_normal_1.fastq.gz",
        r2 = "data/{sample_group}_normal_2.fastq.gz"
    output:
        tsv = "results/{sample_group}/hla/{sample_group}_normal_result.tsv",
    params:
        prefix = lambda wc: f"{wc.sample_group}_normal",
        mode_flag = "--dna",
        conda_base = "/home/longyh/miniforge3",
        optitype_env = OPTITYPE_ENV
    threads: 1
    log: "logs/{sample_group}/hla_typing.log"
    shell:
        """
        set -euo pipefail
        mkdir -p results/{wildcards.sample_group}/hla "$(dirname {log})"
        exec > >(tee -a {log}) 2>&1

        # 初始化并激活 conda
        if [ -f "{params.conda_base}/etc/profile.d/conda.sh" ]; then
          source "{params.conda_base}/etc/profile.d/conda.sh"
        else
          eval "$({params.conda_base}/bin/conda shell.bash hook)"
        fi
        conda activate "{params.optitype_env}"

        SG="{wildcards.sample_group}"
        OUTDIR="results/$SG/hla"
        PREFIX="{params.prefix}"

        echo "[INFO] Using OptiType at: $(which OptiTypePipeline.py)"

        OptiTypePipeline.py \
          --input {input.r1} {input.r2} \
          {params.mode_flag} \
          --outdir "$OUTDIR" \
          --prefix "$PREFIX" \
          --enumerate 2

        conda deactivate
        """

# 4. Mutect2 scatter（保持输入匹配，同步优化日志）
rule mutect2_scatter:
    input:
        tumor = lambda wildcards: f"bam/{wildcards.sample_group}/{config['samples'][wildcards.sample_group]['tumor']}.sorted.dedup.rg.bam",
        normal = lambda wildcards: f"bam/{wildcards.sample_group}/{config['samples'][wildcards.sample_group]['normal']}.sorted.dedup.rg.bam"
    output:
        "vcf/{sample_group}/mutect2/{interval_name}.vcf.gz"
    params:
        ref = config["ref_genome"],
        germline = config["germline_resource"],
        pon = config["panel_of_normals"],
        interval_path = lambda wildcards: [p for p in config["genome_intervals"] if os.path.splitext(os.path.basename(p))[0] == wildcards.interval_name][0],
        mutect2_mem = config.get("mem", {}).get("mutect2", "32G"),
        tumor_id = lambda wc: f"{wc.sample_group}_tumor",   # 新增：与对齐时RGSM一致
        normal_id = lambda wc: f"{wc.sample_group}_normal"  # 新增
    threads: config["threads"]["mutect2"]
    log: "logs/{sample_group}/mutect2/{interval_name}.log"
    shell:
        """
        set -euo pipefail
        mkdir -p vcf/{wildcards.sample_group}/mutect2/ $(dirname {log})

        echo "=== 运行GATK Mutect2（区间：{wildcards.interval_name}） ===" >> {log} 2>&1
        gatk --java-options "-Xmx{params.mutect2_mem} -XX:+UseParallelGC" Mutect2 \
            -R {params.ref} \
            -I {input.tumor} -tumor {params.tumor_id} \
            -I {input.normal} -normal {params.normal_id} \
            --germline-resource {params.germline} \
            --panel-of-normals {params.pon} \
            -L {params.interval_path} \
            -O {output} \
            --native-pair-hmm-threads {threads} >> {log} 2>&1

        echo "=== 建立VCF索引（强制覆盖） ===" >> {log} 2>&1
        # 强制覆盖索引，失败时记录但不中断整个rule
        if ! bcftools index -t -f {output} >> {log} 2>&1; then
          echo "[WARN] bcftools index failed for {output}, trying tabix -f ..." >> {log} 2>&1
          if ! tabix -f -p vcf {output} >> {log} 2>&1; then
            echo "[WARN] tabix indexing also failed, proceeding without index" >> {log} 2>&1
          fi
        fi

        echo "=== 完成：VCF文件路径 = {output} ===" >> {log} 2>&1
        """


# 5. Mutect2 gather（合并每个样本组的所有区间VCF）—— 优化日志和内存
def get_interval_names(wildcards):
    """从config的genome_intervals中提取区间名（不含路径和后缀）"""
    return [os.path.splitext(os.path.basename(path))[0] for path in config["genome_intervals"]]

rule mutect2_gather:
    input:
        lambda wc: expand(
            "vcf/{sample_group}/mutect2/{interval_name}.vcf.gz",
            sample_group=wc.sample_group,
            interval_name=get_interval_names(wc)
        )
    output:
        "vcf/{sample_group}/somatic.vcf.gz"
    params:
        merge_mem = config.get("mem", {}).get("merge_vcfs", "32G"),
        ref = config["ref_genome"]
    log: "logs/{sample_group}/mutect2_gather.log"
    shell:
        """
        set -euo pipefail
        mkdir -p vcf/{wildcards.sample_group}/ $(dirname {log})
        echo "=== 运行GATK MergeVcfs（合并区间VCF） ===" >> {log} 2>&1

        INPUT_ARGS=$(printf " -I %s" {input})

        gatk --java-options "-Xmx{params.merge_mem} -XX:+UseParallelGC" MergeVcfs \
            $INPUT_ARGS \
            -O {output} >> {log} 2>&1

        echo "=== 建立合并后VCF索引（强制覆盖） ===" >> {log} 2>&1
        if ! bcftools index -t -f {output} >> {log} 2>&1; then
          echo "[WARN] bcftools index failed for {output}, trying tabix -f ..." >> {log} 2>&1
          tabix -f -p vcf {output} >> {log} 2>&1 || echo "[WARN] tabix indexing also failed; proceeding without index" >> {log} 2>&1
        fi

        echo "=== 完成：合并后VCF路径 = {output} ===" >> {log} 2>&1
        du -sh {output} {output}.tbi >> {log} 2>&1 || true
        """

# 解析与格式化 HLA 列表
def parse_optitype_tsv(tsv_path):
    """
    解析 OptiType 结果 TSV，返回 ['HLA-A*02:01', 'HLA-B*40:01', ...] 列表。
    策略：
      - 取第2行作为首选（OptiType 第一行通常是索引/表头，第二行是最佳结果）
      - 若第2行缺失或为空，回退取第3行
      - 列名固定 A1, A2, B1, B2, C1, C2，有空值则跳过
      - 统一前缀为 HLA-，并保留星号与冒号（pVACseq 期望 HLA-A*02:01 格式）
      - 去重、稳定排序（A, B, C 顺序）
    """
    import csv
    alleles = []
    try:
        with open(tsv_path, newline='') as f:
            reader = csv.DictReader(f, delimiter='\t')
            rows = list(reader)
            if not rows:
                return []
            # 优先使用第1个数据行（在 DictReader 下就是 rows[0]）
            row = rows[0]
            # 如果有第二个候选行且第一行某些位点为空，尝试用第二行补全
            if len(rows) > 1:
                fallback = rows[1]
            else:
                fallback = None

            for locus in ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']:
                val = (row.get(locus) or '').strip()
                if not val and fallback:
                    val = (fallback.get(locus) or '').strip()
                if val:
                    # OptiType 输出如 A*02:01，需要加 HLA- 前缀并保留格式
                    if not val.startswith('HLA-'):
                        val = f'HLA-{val}'
                    alleles.append(val)

        # 去重并按基因座排序（A,B,C 优先，内部按字典序）
        def sort_key(h):
            # HLA-A*02:01 -> ('A', 'A*02:01')
            try:
                gene = h.split('-')[1].split('*')[0]  # A, B, C
            except Exception:
                gene = 'Z'
            return (['A', 'B', 'C'].index(gene) if gene in ['A', 'B', 'C'] else 9, h)

        unique = sorted(set(alleles), key=sort_key)
        return unique
    except Exception:
        return []

# 6. HLA结果格式化
rule hla_format:
    input:
        tsv = "results/{sample_group}/hla/{sample_group}_normal_result.tsv"
    output:
        alleles = "results/{sample_group}/hla/{sample_group}.alleles.txt"
    log:
        "logs/{sample_group}/hla_format.log"
    run:
        import os
        os.makedirs(os.path.dirname(str(output.alleles)), exist_ok=True)
        alleles = parse_optitype_tsv(str(input.tsv))
        with open(str(log), 'w') as lg:
            if not alleles:
                lg.write(f"ERROR: No HLA alleles parsed from {input.tsv}\n")
                raise ValueError(f"No HLA alleles parsed from {input.tsv}")
            lg.write(f"[INFO] Parsed alleles: {','.join(alleles)}\n")
        with open(str(output.alleles), 'w') as out:
            out.write(','.join(alleles) + '\n')

# 7. VEP注释
rule vep_annotate:
    input:
        vcf = "vcf/{sample_group}/somatic.vcf.gz"
    output:
        vcf = "vcf/{sample_group}/somatic.vep.vcf.gz",
        tbi = "vcf/{sample_group}/somatic.vep.vcf.gz.tbi"
    params:
        conda_base = "/home/longyh/miniforge3",
        pvac_env = PVACTOOLS_ENV,
        cache_dir = config["vep"]["cache_dir"],
        assembly = config["vep"]["assembly"],
        fasta = config.get("ref_genome"),
        vep_mem = config.get("mem", {}).get("vep", "32G")
    threads: config.get("threads", {}).get("vep", 8)
    log: "logs/{sample_group}/vep_annotate.log"
    shell:
        """
        set -euo pipefail
        mkdir -p vcf/{wildcards.sample_group}/ $(dirname {log}) tmp/{wildcards.sample_group}
        exec > >(tee -a {log}) 2>&1

        set +u
        if [ -f "{params.conda_base}/etc/profile.d/conda.sh" ]; then
          source "{params.conda_base}/etc/profile.d/conda.sh"
        else
          eval "$({params.conda_base}/bin/conda shell.bash hook)"
        fi
        conda activate "{params.pvac_env}"
        set -u

        echo "[INFO] Using VEP: $(which vep || echo 'NOT FOUND')"
        echo "[INFO] Input VCF: {input.vcf}"

        if [ ! -s {input.vcf} ]; then
            echo "ERROR: 输入VCF缺失或为空: {input.vcf}"
            conda deactivate || true
            exit 1
        fi

        FASTA_ARG=""
        if [ -n "{params.fasta}" ] && [ "{params.fasta}" != "None" ]; then
          FASTA_ARG="--fasta {params.fasta}"
        fi

        RAW_OUT="tmp/{wildcards.sample_group}/vep_raw.vcf.gz"
        SORT_TMP="tmp/{wildcards.sample_group}/vep_sort_tmp"
        mkdir -p "$SORT_TMP"

        echo "[INFO] Running VEP with threads={threads}, assembly={params.assembly}"
        vep \
          --input_file {input.vcf} \
          --output_file "$RAW_OUT" \
          --vcf \
          --compress_output bgzip \
          --cache \
          --offline \
          --dir_cache {params.cache_dir} \
          --dir_plugins {params.cache_dir}/Plugins \
          --assembly {params.assembly} \
          --fork {threads} \
          --no_stats \
          --verbose \
          --max_af \
          --symbol \
          --hgvs \
          --canonical \
          --af \
          --af_1kg \
          --af_gnomad \
          --variant_class \
          --polyphen b \
          --sift b \
          --distance 500 \
          --protein \
          --tsl \
          --plugin Wildtype \
          --plugin Frameshift \
          --force_overwrite \
          $FASTA_ARG \
          2>> {log}

        if ! bcftools view -h "$RAW_OUT" | grep -q 'ID=CSQ'; then
          echo "ERROR: VEP 原始输出未包含 CSQ 注释头：$RAW_OUT"
          conda deactivate || true
          exit 1
        fi

        bcftools sort -m {params.vep_mem} -T "$SORT_TMP" "$RAW_OUT" | \
          bgzip -@ {threads} -c > {output.vcf}

        if ! bcftools index -t -f {output.vcf}; then
          echo "[WARN] bcftools index failed, trying tabix -f ..."
          tabix -f -p vcf {output.vcf} || echo "[WARN] tabix indexing failed"
        fi

        if ! bcftools view -h {output.vcf} | grep -q 'ID=CSQ'; then
          echo "ERROR: 生成的VCF未包含CSQ注释头：{output.vcf}"
          conda deactivate || true
          exit 1
        fi
        echo "[INFO] VEP annotation completed: {output.vcf}"
        set +u
        conda deactivate || true
        set -u
        """


# 8a. 新抗原预测（运行 pvacseq，不检查具体输出文件名）
rule pvacseq_run:
    input:
        vcf = "vcf/{sample_group}/somatic.vep.vcf.gz",
        hla_list = "results/{sample_group}/hla/{sample_group}.alleles.txt"
    output:
        # 产出一个完成标记文件，代表 run 完成，避免尾部文件名差异导致失败
        done = "results/{sample_group}/pvacseq_vep/pvacseq.done"
    params:
        tumor_id = lambda wc: f"{wc.sample_group}_tumor",
        netmhc_dir = config["netmhc_dir"],
        epitope_lengths = config["pvacseq"]["epitope_lengths"],
        conda_base = "/home/longyh/miniforge3",
        pvac_env = PVACTOOLS_ENV,
        outdir = lambda wc: f"results/{wc.sample_group}/pvacseq_vep"
    threads: config["threads"]["pvacseq"]
    log: "logs/{sample_group}/pvacseq_run.log"
    shell:
        """
        set -euo pipefail
        mkdir -p {params.outdir} $(dirname {log})
        exec > >(tee -a {log}) 2>&1

        set +u
        if [ -f "{params.conda_base}/etc/profile.d/conda.sh" ]; then
          source "{params.conda_base}/etc/profile.d/conda.sh"
        else
          eval "$({params.conda_base}/bin/conda shell.bash hook)"
        fi
        conda activate "{params.pvac_env}"
        set -u

        echo "[DEBUG] pvacseq path: $(which pvacseq || echo 'NOT FOUND')"
        echo "[DEBUG] IEDB dir: {params.netmhc_dir}"
        ls -l {params.netmhc_dir} || true

        if [ ! -s {input.vcf} ]; then
            echo "ERROR: VCF missing {input.vcf}"
            conda deactivate || true
            exit 1
        fi
        if ! bcftools view -h {input.vcf} | grep -q 'ID=CSQ'; then
            echo "ERROR: 输入VCF缺少CSQ注释头（未通过VEP注释？）: {input.vcf}"
            conda deactivate || true
            exit 1
        fi
        if [ ! -s {input.hla_list} ]; then
            echo "ERROR: HLA list missing {input.hla_list}"
            conda deactivate || true
            exit 1
        fi

        if ! bcftools query -l {input.vcf} | grep -Fxq "{params.tumor_id}"; then
            echo "ERROR: 期望样本名 {params.tumor_id} 不在 VCF 样本列中: $(bcftools query -l {input.vcf} | tr '\\n' ',')"
            conda deactivate || true
            exit 1
        fi

        HLA_ALLELES=$(tr -d '\\n' < {input.hla_list})
        echo "[INFO] Using HLA alleles: $HLA_ALLELES"

        # 独立输出目录，避免与非VEP输入的历史缓存冲突（若存在则直接复用以支持增量）
        OUTDIR="{params.outdir}"
        mkdir -p "$OUTDIR"

        pvacseq run \
            {input.vcf} \
            {params.tumor_id} \
            "$HLA_ALLELES" \
            NetMHCpan \
            "$OUTDIR" \
            -e1 {params.epitope_lengths} \
            --iedb-install-directory {params.netmhc_dir} \
            --n-threads {threads} \
            --net-chop-method cterm \
            --netmhc-stab \
            --pass-only

        # 生成完成标记（不依赖具体文件名）
        echo "$(date) pvacseq run completed for {params.tumor_id}" > {output.done}

        set +u
        conda deactivate || true
        set -u
        """

# 8b. 新抗原结果收集（兼容 .final.tsv 与 .tsv；标记缺失时不强制重算）
rule pvacseq_collect:
    # 不再把 done 作为硬输入；改为通过 shell 逻辑自检
    input:
        # 可保留为空或依赖上游的 hla_format/vep_annotate 产物，按需
    output:
        "results/{sample_group}/final.neoantigens.tsv"
    params:
        tumor_id = lambda wc: f"{wc.sample_group}_tumor",
        outdir = lambda wc: f"results/{wc.sample_group}/pvacseq_vep",
        conda_base = "/home/longyh/miniforge3",   # 用于可选补标记时
        pvac_env = PVACTOOLS_ENV
    log: "logs/{sample_group}/pvacseq_collect.log"
    shell:
        """
        set -euo pipefail
        mkdir -p $(dirname {log})
        exec > >(tee -a {log}) 2>&1

        OUTDIR="{params.outdir}"
        PVAC_OUTPUT_FINAL="$OUTDIR/MHC_Class_I/{params.tumor_id}.final.tsv"
        PVAC_OUTPUT_RAW="$OUTDIR/MHC_Class_I/{params.tumor_id}.tsv"
        DONE="$OUTDIR/pvacseq.done"

        echo "[INFO] Collecting from $OUTDIR"

        if [ -f "$PVAC_OUTPUT_FINAL" ]; then
            echo "[INFO] Found final output: $PVAC_OUTPUT_FINAL"
            cp "$PVAC_OUTPUT_FINAL" {output}
        elif [ -f "$PVAC_OUTPUT_RAW" ]; then
            echo "[INFO] Found raw output (no .final suffix): $PVAC_OUTPUT_RAW"
            cp "$PVAC_OUTPUT_RAW" {output}
        else
            echo "[ERROR] pVACseq outputs not found (checked: $PVAC_OUTPUT_FINAL and $PVAC_OUTPUT_RAW)"
            echo "[HINT] 请先运行 pvacseq_run 或确保目录中已有 pVACseq 结果文件。"
            ls -l "$OUTDIR/MHC_Class_I" || true
            exit 1
        fi

        # 若没有完成标记则自动补齐（避免下次再触发重算）
        if [ ! -f "$DONE" ]; then
          echo "[INFO] Creating completion marker: $DONE"
          echo "$(date) pvacseq outputs present; marking done." > "$DONE"
        fi

        # 正则化清理 *.tsv_[0-9]+-[0-9]+
        CLEANUP="{config.get('pvacseq', {}).get('cleanup_temp', True)}"
        if [ "$CLEANUP" = "True" ] || [ "$CLEANUP" = "true" ] || [ "$CLEANUP" = "1" ]; then
          echo "[INFO] Cleaning shard files (*.tsv_[0-9]+-[0-9]+) in $OUTDIR/MHC_Class_I"
          find "$OUTDIR/MHC_Class_I" -maxdepth 1 -type f \
            -regextype posix-extended \
            -regex '.+\.tsv_[0-9]+-[0-9]+$' \
            -print -delete || true
        else
          echo "[INFO] Skipping temp cleanup per config."
        fi
        

        # 可选工件收集（若存在则复制）
        # ART_DIR="results/{wildcards.sample_group}/pvacseq_artifacts"
        # mkdir -p "$ART_DIR"

        # for f in \
        #   "$OUTDIR/MHC_Class_I/{params.tumor_id}.MHC_I.filtered.tsv" \
        #   "$OUTDIR/MHC_Class_I/{params.tumor_id}.MHC_I.all_epitopes.tsv" \
        #   "$OUTDIR/MHC_Class_I/{params.tumor_id}.MHC_I.all_epitopes.aggregated.tsv" \
        #   "$OUTDIR/MHC_Class_I/{params.tumor_id}.fasta" \
        #   "$OUTDIR/MHC_Class_I/{params.tumor_id}.net_chop.fa" \
        # ; do
        #   if [ -f "$f" ]; then
        #     echo "[INFO] Collecting artifact: $(basename "$f")"
        #     cp -f "$f" "$ART_DIR/"
        #   else
        #     echo "[WARN] Optional artifact missing: $f"
        #   fi
        # done


        """


# 9. IC50结合亲和力汇总
rule extract_ic50:
    input: "results/{sample_group}/final.neoantigens.tsv"
    output: "results/{sample_group}/ic50_binding_affinity.tsv"
    log: "logs/{sample_group}/extract_ic50.log"
    shell:
        """
        set -euo pipefail
        mkdir -p $(dirname {log})
        awk -F'\t' '
            NR==1 {{
                ic50_col=-1; mut_col=-1; hla_col=-1; epi_col=-1;
                for(i=1;i<=NF;i++) {{
                    if ($i ~ /IC50/) ic50_col = i
                    if ($i ~ /Mutation/) mut_col = i
                    if ($i ~ /HLA/) hla_col = i
                    if ($i ~ /Epitope/) epi_col = i
                }}
                if (ic50_col == -1 || mut_col == -1 || hla_col == -1 || epi_col == -1) {{
                    echo "ERROR: Required columns not found in input!" && exit 1
                }}
                print "mutation_id\thla_allele\tepitope_sequence\tic50_nM"
            }}
            NR>1 {{
                print $mut_col "\t" $hla_col "\t" $epi_col "\t" $ic50_col
            }}
        ' {input} > {output} 2>&1 | tee {log}
        if [ ! -s {output} ]; then
            echo "ERROR: IC50 output for {wildcards.sample_group} is empty!" && exit 1
        fi
        """

