configfile: "config.yaml"

NEO_ENV = "/home/longyh/miniforge3/envs/neo"  # 替换为你的neo环境路径
PVACTOOLS_ENV = "/home/longyh/miniforge3/envs/pvactools"  # 替换为你的pvactools路径

# 全局参数校验
import os
assert os.path.exists(config["ref_genome"]), f"Ref genome {config['ref_genome']} not found!"
assert os.path.exists(config["germline_resource"]), "Germline resource not found!"
assert os.path.exists(config["panel_of_normals"]), "PON not found!"
assert "epitope_lengths" in config["pvacseq"], "请在config.yaml的pvacseq下配置epitope_lengths（如8,9,10,11）"
assert "netmhc_dir" in config, "请在config.yaml中配置netmhc_dir路径"
assert "genome_intervals" in config, "请在config.yaml中配置genome_intervals（手动指定区间文件列表）"

# 提取所有样本组名（从config中）
SAMPLE_GROUPS = list(config["samples"].keys())

rule all:
    input:
        # 每个样本组的结果文件都带样本组名
        expand("results/{sample_group}/final.neoantigens.tsv", sample_group=SAMPLE_GROUPS),
        expand("results/{sample_group}/ic50_binding_affinity.tsv", sample_group=SAMPLE_GROUPS),
        # 每个样本组的fastqc结果（tumor/normal都带样本组名）
        expand("qc/fastqc/{sample_group}_{sample}_1_fastqc.html", 
               sample_group=SAMPLE_GROUPS, sample=["tumor", "normal"])

# 1. 质控：FastQC
rule fastqc:
    input:
        # 输入fastq路径：data/{样本组}_{tumor/normal}_1.fastq.gz（需对齐你的数据命名）
        r1 = "data/{sample_group}_{sample}_1.fastq.gz",
        r2 = "data/{sample_group}_{sample}_2.fastq.gz"
    output:
        # 输出带样本组名，避免覆盖
        html1 = "qc/fastqc/{sample_group}_{sample}_1_fastqc.html",
        html2 = "qc/fastqc/{sample_group}_{sample}_2_fastqc.html"
    threads: config["threads"]["fastqc"]
    log: "logs/{sample_group}/fastqc/{sample}.log"  # 日志也按样本组分
    shell:
        """
        set -euo pipefail
        mkdir -p logs/{sample_group}/fastqc/  # 确保样本组日志目录存在
        fastqc -t {threads} {input.r1} {input.r2} -o qc/fastqc/ 2>&1 | tee {log}
        """

# 2. 比对 + 标记重复
rule align_and_dedup:
    input:
        r1 = "data/{sample_group}_{sample}_1.fastq.gz",
        r2 = "data/{sample_group}_{sample}_2.fastq.gz"
    output:
        # BAM文件按样本组存放，避免多样本覆盖
        bam = "bam/{sample_group}/{sample}.sorted.dedup.bam",
        bai = "bam/{sample_group}/{sample}.sorted.dedup.bam.bai"
    threads: config["threads"]["align"]
    params:
        ref = config["ref_genome"],
        tmp_dir = "tmp/{sample_group}/{sample}"
    log: "logs/{sample_group}/align/{sample}.log"
    shell:
        """
        set -euo pipefail
        mkdir -p {params.tmp_dir} logs/{sample_group}/align/ bam/{sample_group}/
        bwa-mem2 mem -t {threads} {params.ref} {input.r1} {input.r2} | \
        sambamba view -S -f bam /dev/stdin | \
        sambamba sort -t {threads} --tmpdir {params.tmp_dir} -o {output.bam} --compression-level 6 /dev/stdin 2>&1 | tee {log}
        sambamba markdup -t {threads} --remove-duplicates {output.bam} {output.bam}.tmp
        mv {output.bam}.tmp {output.bam}
        sambamba index -t {threads} {output.bam}
        rm -rf {params.tmp_dir}
        """

# 3. HLA分型（按样本组运行，每个样本组单独出结果）
rule hla_typing:
    input:
        r1=lambda wc: f"data/{config['samples'][wc.sample_group]['tumor']}_1.fastq.gz",
        r2=lambda wc: f"data/{config['samples'][wc.sample_group]['tumor']}_2.fastq.gz"
    output:
        tsv="results/{sample_group}/hla/results.tsv",
        genotype="results/{sample_group}/hla/{sample_group}_HLA_genotype.txt"
    params:
        sample_name=lambda wc: config['samples'][wc.sample_group]['tumor'],
        outdir="results/{sample_group}/hla"
    threads: config["threads"]["hla"]
    log: "logs/{sample_group}/hla_typing.log"
    container: "docker://fred2/optitype"
    shell:
        """
        set -euo pipefail
        mkdir -p {params.outdir}

        optitype_pipeline.py \
            -i {input.r1} {input.r2} \
            -o {params.outdir} \
            --sample-name {params.sample_name} \
            --threads {threads} \
            --enumerate 2 \
            -d 2>&1 | tee {log}

        [ -f {params.outdir}/results.tsv ] && mv {params.outdir}/results.tsv {output.tsv}
        cp {params.outdir}/{params.sample_name}_result.txt {output.genotype}
        """

# 4. Mutect2 scatter（按样本组+手动区间拆分）
rule mutect2_scatter:
    input:
        # 按样本组从config中获取tumor/normal样本名，匹配bam文件
        tumor = lambda wildcards: f"bam/{wildcards.sample_group}/{config['samples'][wildcards.sample_group]['tumor']}.sorted.dedup.bam",
        normal = lambda wildcards: f"bam/{wildcards.sample_group}/{config['samples'][wildcards.sample_group]['normal']}.sorted.dedup.bam"
    output:
        "vcf/{sample_group}/mutect2/{interval_name}.vcf.gz"
    params:
        ref = config["ref_genome"],
        germline = config["germline_resource"],
        pon = config["panel_of_normals"],
        # 从config的genome_intervals中匹配当前interval_name对应的完整路径
        interval_path = lambda wildcards: [p for p in config["genome_intervals"] if os.path.splitext(os.path.basename(p))[0] == wildcards.interval_name][0]
    threads: config["threads"]["mutect2"]
    log: "logs/{sample_group}/mutect2/{interval_name}.log"
    shell:
        """
        set -euo pipefail
        mkdir -p vcf/{sample_group}/mutect2/ logs/{sample_group}/mutect2/
        gatk Mutect2 \
            -R {params.ref} \
            -I {input.tumor} -tumor tumor \
            -I {input.normal} -normal normal \
            --germline-resource {params.germline} \
            --panel-of-normals {params.pon} \
            -L {params.interval_path} \
            -O {output} \
            --native-pair-hmm-threads {threads} 2>&1 | tee {log}
        tabix -p vcf {output}
        """

# 5. Mutect2 gather（合并每个样本组的所有区间VCF）
def get_interval_names(wildcards):
    """从config的genome_intervals中提取区间名（不含路径和后缀）"""
    return [os.path.splitext(os.path.basename(path))[0] for path in config["genome_intervals"]]

rule mutect2_gather:
    input:
        expand("vcf/{sample_group}/mutect2/{interval_name}.vcf.gz", 
               sample_group=SAMPLE_GROUPS,
               interval_name=get_interval_names)
    output:
        "vcf/{sample_group}/somatic.vcf.gz"
    log: "logs/{sample_group}/mutect2_gather.log"
    shell:
        """
        set -euo pipefail
        mkdir -p vcf/{sample_group}/ logs/{sample_group}/
        gatk MergeVcfs -I {input} -O {output} 2>&1 | tee {log}
        tabix -p vcf {output}
        """

# 6. 新抗原预测
rule pvacseq:
    input:
        vcf = "vcf/{sample_group}/somatic.vcf.gz",
        hla = "results/{sample_group}/hla/results.tsv"  # 匹配每个样本组的HLA结果
    output:
        "results/{sample_group}/final.neoantigens.tsv"
    params:
        sample_name = "{sample_group}",
        netmhc_dir = config["netmhc_dir"],
        epitope_lengths = config["pvacseq"]["epitope_lengths"]  # 关键修复：用=代替:
    threads: config["threads"]["pvacseq"]
    log: "logs/{sample_group}/pvacseq.log"
    shell:
        """
        set -euo pipefail
        # 激活pvactools环境（建议用绝对路径避免冲突）
        source activate {PVACTOOLS_ENV}
        mkdir -p results/{sample_group}/ logs/{sample_group}/
        if [ ! -s {input.hla} ]; then
            echo "ERROR: HLA typing result {input.hla} is empty!" && exit 1
        fi
        # 提取HLA等位基因（兼容Optitype输出格式）
        HLA_ALLELES=$(awk -F'\t' '
            NR==2 {{
                for(i=2;i<=7;i++) {{
                    if ($i != "") {{
                        gsub(/\*/, "", $i);
                        printf "%s,", $i
                    }}
                }}
            }}
        ' {input.hla} | sed 's/,$//')
        if [ -z "$HLA_ALLELES" ]; then
            echo "ERROR: Failed to extract HLA alleles for {sample_group}!" && exit 1
        fi
        # 运行pvacseq
        pvacseq run \
            {input.vcf} \
            {params.sample_name} \
            "$HLA_ALLELES" \
            NetMHCpan \
            results/{sample_group}/pvacseq/ \
            -e {params.epitope_lengths} \
            --iedb-install-directory {params.netmhc_dir} \
            --n-threads {threads} \
            --net-chop-method cterm \
            --netmhc-stab \
            --pass-only 2>&1 | tee {log}
        # 复制最终结果（兼容pvacseq输出路径）
        PVAC_OUTPUT="results/{sample_group}/pvacseq/MHC_Class_I/{params.sample_name}.final.tsv"
        if [ ! -f "$PVAC_OUTPUT" ]; then
            echo "ERROR: pVACseq output $PVAC_OUTPUT not found for {sample_group}!" && exit 1
        fi
        cp "$PVAC_OUTPUT" {output}
        # 退出环境
        source deactivate
        """

# 7. IC50结合亲和力汇总
rule extract_ic50:
    input: "results/{sample_group}/final.neoantigens.tsv"
    output: "results/{sample_group}/ic50_binding_affinity.tsv"
    log: "logs/{sample_group}/extract_ic50.log"
    shell:
        """
        set -euo pipefail
        mkdir -p logs/{sample_group}/
        awk -F'\t' '
            NR==1 {{
                ic50_col=-1; mut_col=-1; hla_col=-1; epi_col=-1;
                for(i=1;i<=NF;i++) {{
                    if ($i ~ /IC50/) ic50_col = i
                    if ($i ~ /Mutation/) mut_col = i
                    if ($i ~ /HLA/) hla_col = i
                    if ($i ~ /Epitope/) epi_col = i
                }}
                if (ic50_col == -1 || mut_col == -1 || hla_col == -1 || epi_col == -1) {{
                    echo "ERROR: Required columns not found in input!" && exit 1
                }}
                print "mutation_id\thla_allele\tepitope_sequence\tic50_nM"
            }}
            NR>1 {{
                print $mut_col "\t" $hla_col "\t" $epi_col "\t" $ic50_col
            }}
        ' {input} > {output} 2>&1 | tee {log}
        if [ ! -s {output} ]; then
            echo "ERROR: IC50 output for {sample_group} is empty!" && exit 1
        fi
        """

