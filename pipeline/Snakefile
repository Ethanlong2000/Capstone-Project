configfile: "config.yaml"

NEO_ENV = "/home/longyh/miniforge3/envs/neo"  # 替换为你的neo环境路径
PVACTOOLS_ENV = "/home/longyh/miniforge3/envs/pvactools"  # 替换为你的pvactools路径
OPTITYPE_ENV = "/home/longyh/miniforge3/envs/optitype"  # OptiType 环境路径

# 全局参数校验
import os, glob, re
assert os.path.exists(config["ref_genome"]), f"Ref genome {config['ref_genome']} not found!"
assert os.path.exists(config["germline_resource"]), "Germline resource not found!"
assert os.path.exists(config["panel_of_normals"]), "PON not found!"
assert "epitope_lengths" in config["pvacseq"], "请在config.yaml的pvacseq下配置epitope_lengths（如8,9,10,11）"
assert "netmhc_dir" in config, "请在config.yaml中配置netmhc_dir路径"
assert "genome_intervals" in config, "请在config.yaml中配置genome_intervals（手动指定区间文件列表）"

# 自动发现样本组（当 samples 为 'auto' 或空时）
def autodiscover_samples(data_dir="data"):
    groups = {}
    for r1 in sorted(glob.glob(os.path.join(data_dir, "*_tumor_1.fastq.gz"))):
        base = os.path.basename(r1)
        grp = re.sub(r"_tumor_1\.fastq\.gz$", "", base)
        t1 = os.path.join(data_dir, f"{grp}_tumor_1.fastq.gz")
        t2 = os.path.join(data_dir, f"{grp}_tumor_2.fastq.gz")
        n1 = os.path.join(data_dir, f"{grp}_normal_1.fastq.gz")
        n2 = os.path.join(data_dir, f"{grp}_normal_2.fastq.gz")
        if all(os.path.exists(p) for p in (t1, t2, n1, n2)):
            # 文件名中样本名就是 tumor/normal
            groups[grp] = {"tumor": "tumor", "normal": "normal"}
    return groups

if not isinstance(config.get("samples", {}), dict) or config.get("samples") in (None, "auto") or len(config.get("samples", {})) == 0:
    config["samples"] = autodiscover_samples()
    assert config["samples"], "自动发现失败：请确保 data/ 下存在 {group}_{tumor|normal}_{1,2}.fastq.gz 成对文件。"

# 提取所有样本组名（从config或自动发现）
# 自动发现/读取完成后
ALL_GROUPS = list(config["samples"].keys())

# 允许用 --config sample_groups=Pt8,Pt9 或环境变量 SAMPLE_GROUPS=Pt8,Pt9 来筛选
_selected = config.get("sample_groups") or os.environ.get("SAMPLE_GROUPS")
if _selected:
    SAMPLE_GROUPS = [g.strip() for g in str(_selected).split(",") if g.strip()]
    unknown = [g for g in SAMPLE_GROUPS if g not in ALL_GROUPS]
    assert not unknown, f"未知样本组: {unknown}；可选: {ALL_GROUPS}"
else:
    SAMPLE_GROUPS = ALL_GROUPS

rule all:
    input:
        # 每个样本组的结果文件都带样本组名
        expand("results/{sample_group}/final.neoantigens.tsv", sample_group=SAMPLE_GROUPS),
        expand("results/{sample_group}/ic50_binding_affinity.tsv", sample_group=SAMPLE_GROUPS),
        # 每个样本组的fastqc结果（tumor/normal都带样本组名）
        expand("qc/fastqc/{sample_group}_{sample}_1_fastqc.html", 
               sample_group=SAMPLE_GROUPS, sample=["tumor", "normal"])

# 1. 质控：FastQC
rule fastqc:
    input:
        r1 = "data/{sample_group}_{sample}_1.fastq.gz",
        r2 = "data/{sample_group}_{sample}_2.fastq.gz"
    output:
        html1 = "qc/fastqc/{sample_group}_{sample}_1_fastqc.html",
        html2 = "qc/fastqc/{sample_group}_{sample}_2_fastqc.html"
    threads: config["threads"]["fastqc"]
    log: "logs/{sample_group}/fastqc/{sample}.log"
    shell:
        """
        set -euo pipefail
        mkdir -p $(dirname {log}) qc/fastqc/
        fastqc -t {threads} {input.r1} {input.r2} -o qc/fastqc/ 2>&1 | tee {log}
        """

# 2. 比对 + 添加 Read Group + 标记重复（优化临时文件名、日志、内存配置）
rule align_and_dedup:
    input:
        r1 = "data/{sample_group}_{sample}_1.fastq.gz",
        r2 = "data/{sample_group}_{sample}_2.fastq.gz"
    output:
        bam = "bam/{sample_group}/{sample}.sorted.dedup.rg.bam",  # 最终输出：排序+去重+带RG
        bai = "bam/{sample_group}/{sample}.sorted.dedup.rg.bam.bai"
    threads: config["threads"]["align"]
    params:
        ref = config["ref_genome"],
        tmp_dir = "tmp/{sample_group}/{sample}",
        # 独立临时文件名（不含最终后缀，避免冗余拼接）
        tmp_sorted = "tmp/{sample_group}/{sample}/aligned.sorted.bam",  # 排序后临时文件
        tmp_rg = "tmp/{sample_group}/{sample}/aligned.sorted.rg.bam",   # 加RG后临时文件
        # RG 参数规范（保持不变）
        rg_id = "{sample_group}_{sample}_RG001",
        rg_sm = "{sample_group}",
        rg_lb = "{sample_group}_{sample}_Lib",
        rg_pl = "ILLUMINA",
        rg_pu = "{sample_group}_{sample}_PU001",
        # 动态内存配置：从config读取，默认8G，你的服务器可设更大（如64G/128G）
        gatk_mem = config.get("mem", {}).get("gatk", "64G")  # 优先用config，无则默认64G
    log: "logs/{sample_group}/align/{sample}.log"
    shell:
        """
        set -euo pipefail
        # 创建临时目录和输出目录（确保存在）
        mkdir -p {params.tmp_dir} $(dirname {log}) bam/{wildcards.sample_group}/

        echo "=== 步骤1：BWA-MEM2比对 + Sambamba排序 ===" >> {log} 2>&1
        # 比对→转BAM→排序：输出到独立临时文件（tmp_sorted）
        bwa-mem2 mem -t {threads} {params.ref} {input.r1} {input.r2} | \
        sambamba view -S -f bam /dev/stdin | \
        sambamba sort -t {threads} --tmpdir {params.tmp_dir} -o {params.tmp_sorted} --compression-level 6 /dev/stdin >> {log} 2>&1

        echo "=== 步骤2：GATK添加Read Group（RG） ===" >> {log} 2>&1
        # 动态内存配置：利用服务器800G内存，避免OOM
        gatk --java-options "-Xmx{params.gatk_mem} -XX:+UseParallelGC" AddOrReplaceReadGroups \
            -I {params.tmp_sorted} \
            -O {params.tmp_rg} \
            -RGID {params.rg_id} \
            -RGSM {params.rg_sm} \
            -RGLB {params.rg_lb} \
            -RGPL {params.rg_pl} \
            -RGPU {params.rg_pu} \
            >> {log} 2>&1

        echo "=== 步骤3：Sambamba标记重复 ===" >> {log} 2>&1
        # 标记重复：输入带RG的临时文件，输出到最终BAM
        sambamba markdup -t {threads} --remove-duplicates {params.tmp_rg} {output.bam} >> {log} 2>&1

        echo "=== 步骤4：建立BAM索引 ===" >> {log} 2>&1
        sambamba index -t {threads} {output.bam} >> {log} 2>&1

        echo "=== 步骤5：清理临时文件 ===" >> {log} 2>&1
        rm -f {params.tmp_sorted} {params.tmp_rg}
        rm -rf {params.tmp_dir}

        echo "=== 完成：BAM文件路径 = {output.bam} ===" >> {log} 2>&1
        # 验证RG是否添加成功（日志中记录结果）
        samtools view -H {output.bam} | grep '@RG' >> {log} 2>&1 || echo "WARNING: RG信息未找到！" >> {log} 2>&1
        """



#3 HLA分型
rule hla_typing:
    input:
        r1 = "data/{sample_group}_normal_1.fastq.gz",
        r2 = "data/{sample_group}_normal_2.fastq.gz"
    output:
        tsv = "results/{sample_group}/hla/{sample_group}_normal_result.tsv",
    params:
        prefix = lambda wc: f"{wc.sample_group}_normal",
        mode_flag = "--dna",
        conda_base = "/home/longyh/miniforge3",
        optitype_env = OPTITYPE_ENV
    threads: 1
    log: "logs/{sample_group}/hla_typing.log"
    shell:
        """
        set -euo pipefail
        mkdir -p results/{wildcards.sample_group}/hla "$(dirname {log})"
        exec > >(tee -a {log}) 2>&1

        # 初始化并激活 conda
        if [ -f "{params.conda_base}/etc/profile.d/conda.sh" ]; then
          source "{params.conda_base}/etc/profile.d/conda.sh"
        else
          eval "$({params.conda_base}/bin/conda shell.bash hook)"
        fi
        conda activate "{params.optitype_env}"

        SG="{wildcards.sample_group}"
        OUTDIR="results/$SG/hla"
        PREFIX="{params.prefix}"

        echo "[INFO] Using OptiType at: $(which OptiTypePipeline.py)"

        OptiTypePipeline.py \
          --input {input.r1} {input.r2} \
          {params.mode_flag} \
          --outdir "$OUTDIR" \
          --prefix "$PREFIX" \
          --enumerate 2

        conda deactivate
        """

# 4. Mutect2 scatter（保持输入匹配，同步优化日志）
rule mutect2_scatter:
    input:
        # 输入与align_and_dedup输出完全匹配
        tumor = lambda wildcards: f"bam/{wildcards.sample_group}/{config['samples'][wildcards.sample_group]['tumor']}.sorted.dedup.rg.bam",
        normal = lambda wildcards: f"bam/{wildcards.sample_group}/{config['samples'][wildcards.sample_group]['normal']}.sorted.dedup.rg.bam"
    output:
        "vcf/{sample_group}/mutect2/{interval_name}.vcf.gz"
    params:
        ref = config["ref_genome"],
        germline = config["germline_resource"],
        pon = config["panel_of_normals"],
        interval_path = lambda wildcards: [p for p in config["genome_intervals"] if os.path.splitext(os.path.basename(p))[0] == wildcards.interval_name][0],
        # Mutect2内存配置：默认32G，可根据config调整
        mutect2_mem = config.get("mem", {}).get("mutect2", "32G")
    threads: config["threads"]["mutect2"]
    log: "logs/{sample_group}/mutect2/{interval_name}.log"
    shell:
        """
        set -euo pipefail
        mkdir -p vcf/{wildcards.sample_group}/mutect2/ $(dirname {log})

        echo "=== 运行GATK Mutect2（区间：{wildcards.interval_name}） ===" >> {log} 2>&1
        # 配置大内存，利用服务器资源
        gatk --java-options "-Xmx{params.mutect2_mem} -XX:+UseParallelGC" Mutect2 \
            -R {params.ref} \
            -I {input.tumor} -tumor {config['samples'][wildcards.sample_group]['tumor']} \
            -I {input.normal} -normal {config['samples'][wildcards.sample_group]['normal']} \
            --germline-resource {params.germline} \
            --panel-of-normals {params.pon} \
            -L {params.interval_path} \
            -O {output} \
            --native-pair-hmm-threads {threads} >> {log} 2>&1

        echo "=== 建立VCF索引 ===" >> {log} 2>&1
        tabix -p vcf {output} >> {log} 2>&1

        echo "=== 完成：VCF文件路径 = {output} ===" >> {log} 2>&1
        """


# 5. Mutect2 gather（合并每个样本组的所有区间VCF）—— 优化日志和内存
def get_interval_names(wildcards):
    """从config的genome_intervals中提取区间名（不含路径和后缀）"""
    return [os.path.splitext(os.path.basename(path))[0] for path in config["genome_intervals"]]

rule mutect2_gather:
    input:
        lambda wc: expand("vcf/{sample_group}/mutect2/{interval_name}.vcf.gz",
                          sample_group=wc.sample_group,
                          interval_name=get_interval_names(wc))
    output:
        "vcf/{sample_group}/somatic.vcf.gz"
    params:
        # 动态内存配置：从config读取，默认32G，可根据需求调整（你的服务器可设64G）
        merge_mem = config.get("mem", {}).get("merge_vcfs", "32G")
    log: "logs/{sample_group}/mutect2_gather.log"
    shell:
        """
        set -euo pipefail
        mkdir -p vcf/{wildcards.sample_group}/ $(dirname {log})

        echo "=== 运行GATK MergeVcfs（合并区间VCF） ===" >> {log} 2>&1
        # 配置大内存，适配大量区间VCF合并场景
        gatk --java-options "-Xmx{params.merge_mem} -XX:+UseParallelGC" MergeVcfs \
            -I {input} \
            -O {output} \
            >> {log} 2>&1

        echo "=== 建立合并后VCF的索引 ===" >> {log} 2>&1
        tabix -p vcf {output} >> {log} 2>&1

        echo "=== 完成：合并后VCF路径 = {output} ===" >> {log} 2>&1
        # 验证输出文件是否有效（日志中记录文件大小）
        du -sh {output} {output}.tbi >> {log} 2>&1
        """

# 6. 新抗原预测
rule pvacseq:
    input:
        vcf = "vcf/{sample_group}/somatic.vcf.gz",
        hla = "results/{sample_group}/hla/{sample_group}_normal_result.tsv"
    output:
        "results/{sample_group}/final.neoantigens.tsv"
    params:
        sample_name = "{sample_group}",
        netmhc_dir = config["netmhc_dir"],
        epitope_lengths = config["pvacseq"]["epitope_lengths"]
    threads: config["threads"]["pvacseq"]
    log: "logs/{sample_group}/pvacseq.log"
    shell:
        """
        set -euo pipefail
        source activate {PVACTOOLS_ENV}
        mkdir -p results/{wildcards.sample_group}/ $(dirname {log})
        if [ ! -s {input.hla} ]; then
            echo "ERROR: HLA typing result {input.hla} is empty!" && exit 1
        fi
        HLA_ALLELES=$(awk -F'\t' '
            NR==2 {{
                for(i=2;i<=7;i++) {{
                    if ($i != "") {{
                        gsub(/\*/, "", $i);
                        printf "%s,", $i
                    }}
                }}
            }}
        ' {input.hla} | sed 's/,$//')
        if [ -z "$HLA_ALLELES" ]; then
            echo "ERROR: Failed to extract HLA alleles for {wildcards.sample_group}!" && exit 1
        fi
        pvacseq run \
            {input.vcf} \
            {params.sample_name} \
            "$HLA_ALLELES" \
            NetMHCpan \
            results/{wildcards.sample_group}/pvacseq/ \
            -e {params.epitope_lengths} \
            --iedb-install-directory {params.netmhc_dir} \
            --n-threads {threads} \
            --net-chop-method cterm \
            --netmhc-stab \
            --pass-only 2>&1 | tee {log}
        PVAC_OUTPUT="results/{wildcards.sample_group}/pvacseq/MHC_Class_I/{params.sample_name}.final.tsv"
        if [ ! -f "$PVAC_OUTPUT" ]; then
            echo "ERROR: pVACseq output $PVAC_OUTPUT not found for {wildcards.sample_group}!" && exit 1
        fi
        cp "$PVAC_OUTPUT" {output}
        source deactivate
        """

# 7. IC50结合亲和力汇总
rule extract_ic50:
    input: "results/{sample_group}/final.neoantigens.tsv"
    output: "results/{sample_group}/ic50_binding_affinity.tsv"
    log: "logs/{sample_group}/extract_ic50.log"
    shell:
        """
        set -euo pipefail
        mkdir -p $(dirname {log})
        awk -F'\t' '
            NR==1 {{
                ic50_col=-1; mut_col=-1; hla_col=-1; epi_col=-1;
                for(i=1;i<=NF;i++) {{
                    if ($i ~ /IC50/) ic50_col = i
                    if ($i ~ /Mutation/) mut_col = i
                    if ($i ~ /HLA/) hla_col = i
                    if ($i ~ /Epitope/) epi_col = i
                }}
                if (ic50_col == -1 || mut_col == -1 || hla_col == -1 || epi_col == -1) {{
                    echo "ERROR: Required columns not found in input!" && exit 1
                }}
                print "mutation_id\thla_allele\tepitope_sequence\tic50_nM"
            }}
            NR>1 {{
                print $mut_col "\t" $hla_col "\t" $epi_col "\t" $ic50_col
            }}
        ' {input} > {output} 2>&1 | tee {log}
        if [ ! -s {output} ]; then
            echo "ERROR: IC50 output for {wildcards.sample_group} is empty!" && exit 1
        fi
        """

