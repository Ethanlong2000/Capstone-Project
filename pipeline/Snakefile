configfile: "config.yaml"

NEO_ENV = "/home/longyh/miniforge3/envs/neo"  # 替换为你的neo环境路径
PVACTOOLS_ENV = "/home/longyh/miniforge3/envs/pvactools"  # 替换为你的pvactools路径
OPTITYPE_ENV = "/home/longyh/miniforge3/envs/optitype"  # OptiType 环境路径

# 全局参数校验
import os, glob, re
assert os.path.exists(config["ref_genome"]), f"Ref genome {config['ref_genome']} not found!"
assert os.path.exists(config["germline_resource"]), "Germline resource not found!"
assert os.path.exists(config["panel_of_normals"]), "PON not found!"
assert "epitope_lengths" in config["pvacseq"], "请在config.yaml的pvacseq下配置epitope_lengths（如8,9,10,11）"
assert "netmhc_dir" in config, "请在config.yaml中配置netmhc_dir路径"
assert "genome_intervals" in config, "请在config.yaml中配置genome_intervals（手动指定区间文件列表）"
assert "vep" in config, "请在config.yaml中添加vep配置块"
assert "cache_dir" in config["vep"], "请在config.yaml的vep下配置cache_dir (VEP缓存目录)"
assert "assembly" in config["vep"], "请在config.yaml的vep下配置assembly (如 GRCh38 或 GRCh37)"
assert os.path.exists(config["vep"]["cache_dir"]), f"VEP cache_dir {config['vep']['cache_dir']} 不存在！"


# 自动发现样本组（当 samples 为 'auto' 或空时）
def autodiscover_samples(data_dir="data"):
    groups = {}
    for r1 in sorted(glob.glob(os.path.join(data_dir, "*_tumor_1.fastq.gz"))):
        base = os.path.basename(r1)
        grp = re.sub(r"_tumor_1\.fastq\.gz$", "", base)
        t1 = os.path.join(data_dir, f"{grp}_tumor_1.fastq.gz")
        t2 = os.path.join(data_dir, f"{grp}_tumor_2.fastq.gz")
        n1 = os.path.join(data_dir, f"{grp}_normal_1.fastq.gz")
        n2 = os.path.join(data_dir, f"{grp}_normal_2.fastq.gz")
        if all(os.path.exists(p) for p in (t1, t2, n1, n2)):
            # 文件名中样本名就是 tumor/normal
            groups[grp] = {"tumor": "tumor", "normal": "normal"}
    return groups

if not isinstance(config.get("samples", {}), dict) or config.get("samples") in (None, "auto") or len(config.get("samples", {})) == 0:
    config["samples"] = autodiscover_samples()
    assert config["samples"], "自动发现失败：请确保 data/ 下存在 {group}_{tumor|normal}_{1,2}.fastq.gz 成对文件。"

# 提取所有样本组名（从config或自动发现）
# 自动发现/读取完成后
ALL_GROUPS = list(config["samples"].keys())

# 允许用 --config sample_groups=Pt8,Pt9 或环境变量 SAMPLE_GROUPS=Pt8,Pt9 来筛选
_selected = config.get("sample_groups") or os.environ.get("SAMPLE_GROUPS")
if _selected:
    SAMPLE_GROUPS = [g.strip() for g in str(_selected).split(",") if g.strip()]
    unknown = [g for g in SAMPLE_GROUPS if g not in ALL_GROUPS]
    assert not unknown, f"未知样本组: {unknown}；可选: {ALL_GROUPS}"
else:
    SAMPLE_GROUPS = ALL_GROUPS

rule all:
    input:
        # 直接输出真正关心的下游结果
        expand("results/{sample_group}/neoantigen_features.tsv", sample_group=SAMPLE_GROUPS),
        # 清理完成标记：确保碎片文件被清理
        expand("results/{sample_group}/pvacseq_vep/cleanup.done", sample_group=SAMPLE_GROUPS),
        # 每个样本组的fastqc结果
        expand("qc/fastqc/{sample_group}_{sample}_1_fastqc.html",
               sample_group=SAMPLE_GROUPS, sample=["tumor", "normal"])


# 0. fastp 修剪（去接头+质量截断）
rule fastp:
    input:
        r1 = "data/{sample_group}_{sample}_1.fastq.gz",
        r2 = "data/{sample_group}_{sample}_2.fastq.gz"
    output:
        r1 = "trimmed/{sample_group}_{sample}_1.fastq.gz",
        r2 = "trimmed/{sample_group}_{sample}_2.fastq.gz",
        json = "qc/fastp/{sample_group}_{sample}.json",
        html = "qc/fastp/{sample_group}_{sample}.html"
    threads: config.get("threads", {}).get("fastp", 8)
    resources:
        io = 4
    log: "logs/{sample_group}/fastp/{sample}.log"
    shell:
        """
        set -euo pipefail
        SECONDS=0
        echo "[START] fastp {wildcards.sample_group}/{wildcards.sample} $(date '+%F %T')" >> {log}
        mkdir -p $(dirname {log}) qc/fastp trimmed
        fastp \
          -i {input.r1} -I {input.r2} \
          -o {output.r1} -O {output.r2} \
          --detect_adapter_for_pe \
          --qualified_quality_phred 20 \
          --length_required 30 \
          --thread {threads} \
          --json {output.json} \
          --html {output.html} \
          > {log} 2>&1
        echo "[END] fastp {wildcards.sample_group}/{wildcards.sample} duration=${SECONDS}s $(date '+%F %T')" >> {log}
        """

# 1. 质控：FastQC
rule fastqc:
    input:
        r1 = "trimmed/{sample_group}_{sample}_1.fastq.gz",
        r2 = "trimmed/{sample_group}_{sample}_2.fastq.gz"
    output:
        html1 = "qc/fastqc/{sample_group}_{sample}_1_fastqc.html",
        html2 = "qc/fastqc/{sample_group}_{sample}_2_fastqc.html"
    threads: config["threads"]["fastqc"]
    log: "logs/{sample_group}/fastqc/{sample}.log"
    shell:
        """
        set -euo pipefail
        SECONDS=0
        echo "[START] fastqc {wildcards.sample_group}/{wildcards.sample} $(date '+%F %T')" >> {log}
        mkdir -p $(dirname {log}) qc/fastqc/
        fastqc -t {threads} {input.r1} {input.r2} -o qc/fastqc/ 2>&1 | tee {log}
        echo "[END] fastqc {wildcards.sample_group}/{wildcards.sample} duration=${SECONDS}s $(date '+%F %T')" >> {log}
        """

# 2. 比对 + 添加 Read Group + 标记重复（优化临时文件名、日志、内存配置）rule align_and_dedup:
    input:
        r1 = "trimmed/{sample_group}_{sample}_1.fastq.gz",
        r2 = "trimmed/{sample_group}_{sample}_2.fastq.gz",
    output:
        bam = "bam/{sample_group}/{sample}.sorted.dedup.rg.bam",
        bai = "bam/{sample_group}/{sample}.sorted.dedup.rg.bam.bai"
    threads: config["threads"]["align"]
    resources:
        io = 4
    params:
        ref = config["ref_genome"],
        tmp_dir = "tmp/{sample_group}/{sample}",
        tmp_sorted = "tmp/{sample_group}/{sample}/aligned.sorted.bam",
        tmp_rg = "tmp/{sample_group}/{sample}/aligned.sorted.rg.bam",
        rg_id = "{sample_group}_{sample}_RG001",
        rg_sm = "{sample_group}_{sample}",
        rg_lb = "{sample_group}_{sample}_Lib",
        rg_pl = "ILLUMINA",
        rg_pu = "{sample_group}_{sample}_PU001",
        gatk_mem = config.get("mem", {}).get("gatk", "64G")
    log: "logs/{sample_group}/align/{sample}.log"
    shell:
        """
        set -euo pipefail
        SECONDS=0
        echo "[START] align_and_dedup {wildcards.sample_group}/{wildcards.sample} $(date '+%F %T')" >> {log}
        mkdir -p {params.tmp_dir} $(dirname {log}) bam/{wildcards.sample_group}/
        # 所有 stdout/stderr 同时写日志并打印到屏幕
        exec > >(tee -a {log}) 2>&1

        echo "=== 步骤1：BWA-MEM2比对 + Sambamba排序 ==="
        bwa-mem2 mem -t {threads} {params.ref} {input.r1} {input.r2} | \
          sambamba view -S -f bam /dev/stdin | \
          sambamba sort -t {threads} --tmpdir {params.tmp_dir} -o {params.tmp_sorted} --compression-level 6 /dev/stdin

        echo "=== 步骤2：GATK添加Read Group（RG） ==="
        gatk --java-options "-Xmx{params.gatk_mem} -XX:+UseParallelGC" AddOrReplaceReadGroups \
            -I {params.tmp_sorted} \
            -O {params.tmp_rg} \
            -RGID {params.rg_id} \
            -RGSM {params.rg_sm} \
            -RGLB {params.rg_lb} \
            -RGPL {params.rg_pl} \
            -RGPU {params.rg_pu}

        echo "=== 步骤3：Sambamba标记重复 ==="
        sambamba markdup -t {threads} --remove-duplicates {params.tmp_rg} {output.bam}

        echo "=== 步骤4：建立BAM索引 ==="
        sambamba index -t {threads} {output.bam}

        echo "=== 步骤5：清理临时文件 ==="
        rm -f {params.tmp_sorted} {params.tmp_rg}
        rm -rf {params.tmp_dir}

        echo "=== 完成：BAM文件路径 = {output.bam} ==="
        samtools view -H {output.bam} | grep '@RG' || echo "WARNING: RG信息未找到！"
        echo "[END] align_and_dedup {wildcards.sample_group}/{wildcards.sample} duration=${SECONDS}s $(date '+%F %T')"
        """



#3 HLA分型
rule hla_typing:
    input:
        r1 = "trimmed/{sample_group}_normal_1.fastq.gz",
        r2 = "trimmed/{sample_group}_normal_2.fastq.gz"
    output:
        tsv = "results/{sample_group}/hla/{sample_group}_normal_result.tsv",
    params:
        prefix = lambda wc: f"{wc.sample_group}_normal",
        mode_flag = "--dna",
        conda_base = "/home/longyh/miniforge3",
        optitype_env = OPTITYPE_ENV
    threads: 1
    resources:
        io = 2
    log: "logs/{sample_group}/hla_typing.log"
    shell:
        """
        set -euo pipefail
        SECONDS=0
        mkdir -p results/{wildcards.sample_group}/hla "$(dirname {log})"
        exec > >(tee -a {log}) 2>&1
        echo "[START] hla_typing {wildcards.sample_group} $(date '+%F %T')"

        # 初始化并激活 conda
        if [ -f "{params.conda_base}/etc/profile.d/conda.sh" ]; then
          source "{params.conda_base}/etc/profile.d/conda.sh"
        else
          eval "$({params.conda_base}/bin/conda shell.bash hook)"
        fi
        conda activate "{params.optitype_env}"

        SG="{wildcards.sample_group}"
        OUTDIR="results/$SG/hla"
        PREFIX="{params.prefix}"

        echo "[INFO] Using OptiType at: $(which OptiTypePipeline.py)"

        OptiTypePipeline.py \
          --input {input.r1} {input.r2} \
          {params.mode_flag} \
          --outdir "$OUTDIR" \
          --prefix "$PREFIX" \
          --enumerate 2

        conda deactivate
        echo "[END] hla_typing {wildcards.sample_group} duration=${SECONDS}s $(date '+%F %T')"
        
        """

# 4. Mutect2 scatter（保持输入匹配，同步优化日志）
rule mutect2_scatter:
    input:
        tumor = lambda wildcards: f"bam/{wildcards.sample_group}/{config['samples'][wildcards.sample_group]['tumor']}.sorted.dedup.rg.bam",
        normal = lambda wildcards: f"bam/{wildcards.sample_group}/{config['samples'][wildcards.sample_group]['normal']}.sorted.dedup.rg.bam"
    output:
        "vcf/{sample_group}/mutect2/{interval_name}.vcf.gz"
    params:
        ref = config["ref_genome"],
        germline = config["germline_resource"],
        pon = config["panel_of_normals"],
        interval_path = lambda wildcards: [p for p in config["genome_intervals"] if os.path.splitext(os.path.basename(p))[0] == wildcards.interval_name][0],
        mutect2_mem = config.get("mem", {}).get("mutect2", "32G"),
        tumor_id = lambda wc: f"{wc.sample_group}_tumor",   # 新增：与对齐时RGSM一致
        normal_id = lambda wc: f"{wc.sample_group}_normal"  # 新增
    threads: config["threads"]["mutect2"]
    resources:
        io = 3
    log: "logs/{sample_group}/mutect2/{interval_name}.log"
    shell:
        """
        set -euo pipefail
        SECONDS=0
        mkdir -p vcf/{wildcards.sample_group}/mutect2/ $(dirname {log})
        echo "[START] mutect2_scatter {wildcards.sample_group} {wildcards.interval_name} $(date '+%F %T')" >> {log}

        echo "=== 运行GATK Mutect2（区间：{wildcards.interval_name}） ===" >> {log} 2>&1
        gatk --java-options "-Xmx{params.mutect2_mem} -XX:+UseParallelGC" Mutect2 \
            -R {params.ref} \
            -I {input.tumor} -tumor {params.tumor_id} \
            -I {input.normal} -normal {params.normal_id} \
            --germline-resource {params.germline} \
            --panel-of-normals {params.pon} \
            -L {params.interval_path} \
            -O {output} \
            --native-pair-hmm-threads {threads} >> {log} 2>&1

        echo "=== 建立VCF索引（强制覆盖） ===" >> {log} 2>&1
        # 强制覆盖索引，失败时记录但不中断整个rule
        if ! bcftools index -t -f {output} >> {log} 2>&1; then
          echo "[WARN] bcftools index failed for {output}, trying tabix -f ..." >> {log} 2>&1
          if ! tabix -f -p vcf {output} >> {log} 2>&1; then
            echo "[WARN] tabix indexing also failed, proceeding without index" >> {log} 2>&1
          fi
        fi

        echo "=== 完成：VCF文件路径 = {output} ===" >> {log} 2>&1
        echo "[END] mutect2_scatter {wildcards.sample_group} {wildcards.interval_name} duration=${SECONDS}s $(date '+%F %T')" >> {log}
        
        """


# 5. Mutect2 gather（合并每个样本组的所有区间VCF）—— 优化日志和内存
def get_interval_names(wildcards):
    """从config的genome_intervals中提取区间名（不含路径和后缀）"""
    return [os.path.splitext(os.path.basename(path))[0] for path in config["genome_intervals"]]

rule mutect2_gather:
    input:
        lambda wc: expand(
            "vcf/{sample_group}/mutect2/{interval_name}.vcf.gz",
            sample_group=wc.sample_group,
            interval_name=get_interval_names(wc)
        )
    output:
        "vcf/{sample_group}/somatic.vcf.gz"
    params:
        merge_mem = config.get("mem", {}).get("merge_vcfs", "32G"),
        ref = config["ref_genome"]
    log: "logs/{sample_group}/mutect2_gather.log"
    resources:
        io = 2
    shell:
        """
        set -euo pipefail
        SECONDS=0
        mkdir -p vcf/{wildcards.sample_group}/ $(dirname {log})
        echo "[START] mutect2_gather {wildcards.sample_group} $(date '+%F %T')" >> {log}

        echo "=== 运行GATK MergeVcfs（合并区间VCF） ===" >> {log} 2>&1

        INPUT_ARGS=$(printf " -I %s" {input})

        gatk --java-options "-Xmx{params.merge_mem} -XX:+UseParallelGC" MergeVcfs \
            $INPUT_ARGS \
            -O {output} >> {log} 2>&1

        echo "=== 建立合并后VCF索引（强制覆盖） ===" >> {log} 2>&1
        if ! bcftools index -t -f {output} >> {log} 2>&1; then
          echo "[WARN] bcftools index failed for {output}, trying tabix -f ..." >> {log} 2>&1
          tabix -f -p vcf {output} >> {log} 2>&1 || echo "[WARN] tabix indexing also failed; proceeding without index" >> {log} 2>&1
        fi

        echo "=== 完成：合并后VCF路径 = {output} ===" >> {log} 2>&1
        du -sh {output} {output}.tbi >> {log} 2>&1 || true
        echo "[END] mutect2_gather {wildcards.sample_group} duration=${SECONDS}s $(date '+%F %T')" >> {log}
        
        """

# 解析与格式化 HLA 列表
def parse_optitype_tsv(tsv_path):
    """
    解析 OptiType 结果 TSV，返回 ['HLA-A*02:01', 'HLA-B*40:01', ...] 列表。
    策略：
      - 取第2行作为首选（OptiType 第一行通常是索引/表头，第二行是最佳结果）
      - 若第2行缺失或为空，回退取第3行
      - 列名固定 A1, A2, B1, B2, C1, C2，有空值则跳过
      - 统一前缀为 HLA-，并保留星号与冒号（pVACseq 期望 HLA-A*02:01 格式）
      - 去重、稳定排序（A, B, C 顺序）
    """
    import csv
    alleles = []
    try:
        with open(tsv_path, newline='') as f:
            reader = csv.DictReader(f, delimiter='\t')
            rows = list(reader)
            if not rows:
                return []
            # 优先使用第1个数据行（在 DictReader 下就是 rows[0]）
            row = rows[0]
            # 如果有第二个候选行且第一行某些位点为空，尝试用第二行补全
            if len(rows) > 1:
                fallback = rows[1]
            else:
                fallback = None

            for locus in ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']:
                val = (row.get(locus) or '').strip()
                if not val and fallback:
                    val = (fallback.get(locus) or '').strip()
                if val:
                    # OptiType 输出如 A*02:01，需要加 HLA- 前缀并保留格式
                    if not val.startswith('HLA-'):
                        val = f'HLA-{val}'
                    alleles.append(val)

        # 去重并按基因座排序（A,B,C 优先，内部按字典序）
        def sort_key(h):
            # HLA-A*02:01 -> ('A', 'A*02:01')
            try:
                gene = h.split('-')[1].split('*')[0]  # A, B, C
            except Exception:
                gene = 'Z'
            return (['A', 'B', 'C'].index(gene) if gene in ['A', 'B', 'C'] else 9, h)

        unique = sorted(set(alleles), key=sort_key)
        return unique
    except Exception:
        return []

# 6. HLA结果格式化
rule hla_format:
    input:
        tsv = "results/{sample_group}/hla/{sample_group}_normal_result.tsv"
    output:
        alleles = "results/{sample_group}/hla/{sample_group}.alleles.txt"
    log:
        "logs/{sample_group}/hla_format.log"
    run:
        import os, datetime
        t0 = datetime.datetime.now()
        os.makedirs(os.path.dirname(str(output.alleles)), exist_ok=True)
        with open(str(log), 'a') as lg:
            lg.write(f"[START] hla_format {wildcards.sample_group} {t0:%F %T}\n")
        alleles = parse_optitype_tsv(str(input.tsv))
        with open(str(log), 'a') as lg:
            if not alleles:
                lg.write(f"ERROR: No HLA alleles parsed from {input.tsv}\n")
                raise ValueError(f"No HLA alleles parsed from {input.tsv}")
            lg.write(f"[INFO] Parsed alleles: {','.join(alleles)}\n")
        with open(str(output.alleles), 'w') as out:
            out.write(','.join(alleles) + '\n')
        with open(str(log), 'a') as lg:
            lg.write(f"[END] hla_format {wildcards.sample_group} duration={(datetime.datetime.now()-t0).total_seconds():.1f}s {datetime.datetime.now():%F %T}\n")


# 7. VEP注释
rule vep_annotate:
    input:
        vcf = "vcf/{sample_group}/somatic.vcf.gz"
    output:
        vcf = "vcf/{sample_group}/somatic.vep.vcf.gz",
        tbi = "vcf/{sample_group}/somatic.vep.vcf.gz.tbi"
    params:
        conda_base = "/home/longyh/miniforge3",
        pvac_env = PVACTOOLS_ENV,
        cache_dir = config["vep"]["cache_dir"],
        assembly = config["vep"]["assembly"],
        fasta = config.get("ref_genome"),
        vep_mem = config.get("mem", {}).get("vep", "32G")
    threads: config.get("threads", {}).get("vep", 8)
    resources:
        io = 2
    log: "logs/{sample_group}/vep_annotate.log"
    shell:
        """
        set -euo pipefail 
        SECONDS=0
        mkdir -p vcf/{wildcards.sample_group}/ $(dirname {log}) tmp/{wildcards.sample_group}
        exec > >(tee -a {log}) 2>&1
        echo "[START] vep_annotate {wildcards.sample_group} $(date '+%F %T')"


        set +u
        if [ -f "{params.conda_base}/etc/profile.d/conda.sh" ]; then
          source "{params.conda_base}/etc/profile.d/conda.sh"
        else
          eval "$({params.conda_base}/bin/conda shell.bash hook)"
        fi
        conda activate "{params.pvac_env}"
        set -u

        echo "[INFO] Using VEP: $(which vep || echo 'NOT FOUND')"
        echo "[INFO] Input VCF: {input.vcf}"

        if [ ! -s {input.vcf} ]; then
            echo "ERROR: 输入VCF缺失或为空: {input.vcf}"
            conda deactivate || true
            exit 1
        fi

        FASTA_ARG=""
        if [ -n "{params.fasta}" ] && [ "{params.fasta}" != "None" ]; then
          FASTA_ARG="--fasta {params.fasta}"
        fi

        RAW_OUT="tmp/{wildcards.sample_group}/vep_raw.vcf.gz"
        SORT_TMP="tmp/{wildcards.sample_group}/vep_sort_tmp"
        mkdir -p "$SORT_TMP"

        echo "[INFO] Running VEP with threads={threads}, assembly={params.assembly}"
        vep \
          --input_file {input.vcf} \
          --output_file "$RAW_OUT" \
          --vcf \
          --compress_output bgzip \
          --cache \
          --offline \
          --dir_cache {params.cache_dir} \
          --dir_plugins {params.cache_dir}/Plugins \
          --assembly {params.assembly} \
          --fork {threads} \
          --no_stats \
          --verbose \
          --max_af \
          --symbol \
          --hgvs \
          --canonical \
          --af \
          --af_1kg \
          --af_gnomad \
          --variant_class \
          --polyphen b \
          --sift b \
          --distance 500 \
          --protein \
          --tsl \
          --plugin Wildtype \
          --plugin Frameshift \
          --force_overwrite \
          $FASTA_ARG \
          2>> {log}

        if ! bcftools view -h "$RAW_OUT" | grep -q 'ID=CSQ'; then
          echo "ERROR: VEP 原始输出未包含 CSQ 注释头：$RAW_OUT"
          conda deactivate || true
          exit 1
        fi

        bcftools sort -m {params.vep_mem} -T "$SORT_TMP" "$RAW_OUT" | \
          bgzip -@ {threads} -c > {output.vcf}

        if ! bcftools index -t -f {output.vcf}; then
          echo "[WARN] bcftools index failed, trying tabix -f ..."
          tabix -f -p vcf {output.vcf} || echo "[WARN] tabix indexing failed"
        fi

        if ! bcftools view -h {output.vcf} | grep -q 'ID=CSQ'; then
          echo "ERROR: 生成的VCF未包含CSQ注释头：{output.vcf}"
          conda deactivate || true
          exit 1
        fi
        echo "[INFO] VEP annotation completed: {output.vcf}"
        set +u
        conda deactivate || true
        set -u
        echo "[END] vep_annotate {wildcards.sample_group} duration=${SECONDS}s $(date '+%F %T')"
        
        """


# 8a. 新抗原预测（运行 pvacseq，不检查具体输出文件名）
rule pvacseq_run:
    input:
        vcf = "vcf/{sample_group}/somatic.vep.vcf.gz",
        hla_list = "results/{sample_group}/hla/{sample_group}.alleles.txt"
    output:
        # 显式声明 pVACseq 的聚合结果，便于下游直接依赖
        aggregated = "results/{sample_group}/pvacseq_vep/MHC_Class_I/{sample_group}_tumor.MHC_I.all_epitopes.aggregated.tsv",
        done = "results/{sample_group}/pvacseq_vep/pvacseq.done"
    params:
        tumor_id = lambda wc: f"{wc.sample_group}_tumor",
        netmhc_dir = config["netmhc_dir"],
        epitope_lengths = config["pvacseq"]["epitope_lengths"],
        conda_base = "/home/longyh/miniforge3",
        pvac_env = PVACTOOLS_ENV,
        outdir = lambda wc: f"results/{wc.sample_group}/pvacseq_vep"
    threads: config["threads"]["pvacseq_run"]
    resources:
        io = 2
    log: "logs/{sample_group}/pvacseq_run.log"
    shell:
        """
        set -euo pipefail
        SECONDS=0
        mkdir -p {params.outdir} $(dirname {log})
        exec > >(tee -a {log}) 2>&1
        echo "[START] pvacseq_run {wildcards.sample_group} $(date '+%F %T')"


        set +u
        if [ -f "{params.conda_base}/etc/profile.d/conda.sh" ]; then
          source "{params.conda_base}/etc/profile.d/conda.sh"
        else
          eval "$({params.conda_base}/bin/conda shell.bash hook)"
        fi
        conda activate "{params.pvac_env}"
        set -u

        echo "[DEBUG] pvacseq path: $(which pvacseq || echo 'NOT FOUND')"
        echo "[DEBUG] IEDB dir: {params.netmhc_dir}"
        ls -l {params.netmhc_dir} || true

        if [ ! -s {input.vcf} ]; then
            echo "ERROR: VCF missing {input.vcf}"
            conda deactivate || true
            exit 1
        fi
        if ! bcftools view -h {input.vcf} | grep -q 'ID=CSQ'; then
            echo "ERROR: 输入VCF缺少CSQ注释头（未通过VEP注释？）: {input.vcf}"
            conda deactivate || true
            exit 1
        fi
        if [ ! -s {input.hla_list} ]; then
            echo "ERROR: HLA list missing {input.hla_list}"
            conda deactivate || true
            exit 1
        fi

        if ! bcftools query -l {input.vcf} | grep -Fxq "{params.tumor_id}"; then
            echo "ERROR: 期望样本名 {params.tumor_id} 不在 VCF 样本列中: $(bcftools query -l {input.vcf} | tr '\\n' ',')"
            conda deactivate || true
            exit 1
        fi

        HLA_ALLELES=$(tr -d '\\n' < {input.hla_list})
        echo "[INFO] Using HLA alleles: $HLA_ALLELES"

        OUTDIR="{params.outdir}"
        mkdir -p "$OUTDIR"

        pvacseq run \
            {input.vcf} \
            {params.tumor_id} \
            "$HLA_ALLELES" \
            NetMHCpan \
            "$OUTDIR" \
            -e1 {params.epitope_lengths} \
            --iedb-install-directory {params.netmhc_dir} \
            --n-threads {threads} \
            --net-chop-method cterm \
            --netmhc-stab \
            --pass-only

        # 生成完成标记
        echo "$(date) pvacseq run completed for {params.tumor_id}" > {output.done}
        # 聚合文件由 pvacseq 自行生成，Snakemake 将以其存在性判定该输出
        if [ ! -s {output.aggregated} ]; then
          echo "ERROR: Expected aggregated output not found: {output.aggregated}"
          conda deactivate || true
          exit 1
        fi

        set +u
        conda deactivate || true
        set -u
        echo "[END] pvacseq_run {wildcards.sample_group} duration=${SECONDS}s $(date '+%F %T')"
        
        """


# 9. 提取新抗原核心特征（仅含可用字段）
rule extract_neoantigen_features:
    input:
        done = "results/{sample_group}/pvacseq_vep/pvacseq.done",
        tsv  = "results/{sample_group}/pvacseq_vep/MHC_Class_I/{sample_group}_tumor.MHC_I.all_epitopes.aggregated.tsv"
    output:
        "results/{sample_group}/neoantigen_features.tsv"
    log:
        "logs/{sample_group}/extract_neoantigen_features.log"
    shell:
        """
        set -euo pipefail
        SECONDS=0
        echo "[START] extract_neoantigen_features {wildcards.sample_group} $(date '+%F %T')" >> {log}

        mkdir -p $(dirname {log}) $(dirname {output})
        awk -F'\t' '
            NR==1 {{
                id_col = -1; allele_col = -1; peptide_col = -1
                ic50_mt_col = -1; ic50_wt_col = -1; dna_vaf_col = -1

                for(i=1; i<=NF; i++) {{
                    if ($i == "ID") id_col = i
                    else if ($i == "Allele") allele_col = i
                    else if ($i == "Best Peptide") peptide_col = i
                    else if ($i == "IC50 MT") ic50_mt_col = i
                    else if ($i == "IC50 WT") ic50_wt_col = i
                    else if ($i == "DNA VAF") dna_vaf_col = i
                }}

                if (id_col == -1 || ic50_mt_col == -1) {{
                    print "ERROR: Critical columns (ID or IC50 MT) missing!" > "/dev/stderr"
                    exit 1
                }}

                print "mutation_id\thla_allele\tepitope_sequence\tic50_mt_nM\tic50_wt_nM\tdna_vaf"
                next
            }}
            {{
                id_val = (id_col > 0) ? $id_col : "NA"
                allele_val = (allele_col > 0) ? $allele_col : "NA"
                peptide_val = (peptide_col > 0) ? $peptide_col : "NA"
                ic50_mt_val = (ic50_mt_col > 0) ? $ic50_mt_col : "NA"
                ic50_wt_val = (ic50_wt_col > 0) ? $ic50_wt_col : "NA"
                dna_vaf_val = (dna_vaf_col > 0) ? $dna_vaf_col : "NA"

                if (allele_val != "NA" && allele_val !~ /^HLA-/) {{
                    allele_val = "HLA-" allele_val
                }}

                print id_val "\t" allele_val "\t" peptide_val "\t" ic50_mt_val "\t" ic50_wt_val "\t" dna_vaf_val
            }}
        ' "{input.tsv}" > "{output}" 2> >(tee "{log}" >&2)

        if [ ! -s "{output}" ]; then
            echo "ERROR: Output file is empty!" >&2
            exit 1
        fi
        echo "[END] extract_neoantigen_features {wildcards.sample_group} duration=${SECONDS}s $(date '+%F %T')" >> {log}
        
        """

# pVACseq 过程文件清理
rule pvacseq_cleanup:
    input:
        done = "results/{sample_group}/pvacseq_vep/pvacseq.done",
        aggregated = "results/{sample_group}/pvacseq_vep/MHC_Class_I/{sample_group}_tumor.MHC_I.all_epitopes.aggregated.tsv",
        features = "results/{sample_group}/neoantigen_features.tsv"
    output:
        "results/{sample_group}/pvacseq_vep/cleanup.done"
    params:
        outdir = lambda wc: f"results/{wc.sample_group}/pvacseq_vep",
        cleanup_flag = lambda wc: str(config.get("pvacseq", {}).get("cleanup_temp", True)).lower()
    log:
        "logs/{sample_group}/pvacseq_cleanup.log"
    shell:
        """
        set -euo pipefail
        SECONDS=0
        OUTDIR="{params.outdir}"
        echo "[START] pvacseq_cleanup {wildcards.sample_group} $(date '+%F %T')" | tee {log}

        CLEANUP="{params.cleanup_flag}"
        if [ "$CLEANUP" = "true" ] || [ "$CLEANUP" = "1" ]; then
          echo "[INFO] Removing shard files (*.tsv_[0-9]+-[0-9]+) under $OUTDIR/MHC_Class_I" | tee -a {log}
          find "$OUTDIR/MHC_Class_I" -maxdepth 1 -type f \
            -regextype posix-extended \
            -regex '.+\\.tsv_[0-9]+-[0-9]+$' \
            -print -delete | tee -a {log} || true
        else
          echo "[INFO] Skipping cleanup per config.pvacseq.cleanup_temp=$CLEANUP" | tee -a {log}
        fi

        date > {output}
        echo "[END] pvacseq_cleanup {wildcards.sample_group} duration=${SECONDS}s $(date '+%F %T')" | tee -a {log}
       
        """
        
